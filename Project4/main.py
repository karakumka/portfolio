from __future__ import annotations
import re
from collections import defaultdict
from zipfile import ZipFile
from lxml import etree
import PyPDF2
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Set, Optional
import spacy
from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
import torch
import pandas as pd
import argparse
from pymorphy3 import MorphAnalyzer
import io
import os
import ru_core_news_sm
import sys
import traceback
import tempfile
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import StreamingResponse

os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# === API ===

app = FastAPI()

def df_to_xlsx_bytes(df: pd.DataFrame) -> io.BytesIO:
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="Sheet1")
    buffer.seek(0)  # –ø–µ—Ä–µ–º–æ—Ç–∞—Ç—å –≤ –Ω–∞—á–∞–ª–æ, —á—Ç–æ–±—ã FastAPI —á–∏—Ç–∞–ª —Å 0
    return buffer

@app.post("/convert")
async def convert_to_xlsx(file: UploadFile = File(...)):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç DOCX –∏–ª–∏ PDF, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç XLSX.
    """
    filename = file.filename or "input"
    ext = os.path.splitext(filename)[1].lower()

    if ext not in [".docx", ".pdf"]:
        raise HTTPException(status_code=400, detail="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ .docx –∏ .pdf")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp:
            tmp.write(await file.read())
            tmp_path = tmp.name
            
        excelDataFrame = full_pipeline_1(tmp_path)
        xlsx_io = df_to_xlsx_bytes(excelDataFrame)

        # –ì–æ—Ç–æ–≤–∏–º –æ—Ç–≤–µ—Ç
        output_name = os.path.splitext(filename)[0] + ".xlsx"
        return StreamingResponse(
            xlsx_io,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={
                "Content-Disposition": f'attachment; filename="{output_name}"'
            },
        )

    finally:
        # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω
        try:
            os.remove(tmp_path)
        except Exception:
            pass
            
@app.get("/ping")
async def health():
    return {"status": "ok"}

# ==== Relative Path ====

def get_base_path() -> Path:
    if getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS"):
        return Path(sys._MEIPASS)
    return Path(__file__).parent

def resource_path(relative: str) -> Path:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ä–µ—Å—É—Ä—Å—É, –∫–æ—Ç–æ—Ä—ã–π –ª–µ–∂–∏—Ç —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º
    (–ø—Ä–∏ –æ–±—ã—á–Ω–æ–º –∑–∞–ø—É—Å–∫–µ) –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏ PyInstaller (_MEIPASS)
    –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –∏–∑ .exe.
    """
    if getattr(sys, "frozen", False) and hasattr(sys, "_MEIPASS"):
        # –ó–∞–ø—É—Å–∫ –∏–∑ —É–ø–∞–∫–æ–≤–∞–Ω–Ω–æ–≥–æ exe
        base_path = Path(sys._MEIPASS)
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ .py
        base_path = Path(__file__).parent

    return base_path / relative
    
BASE_DIR = get_base_path()

def get_device():
    # —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ: –µ—Å–ª–∏ –µ—Å—Ç—å CUDA ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë, –∏–Ω–∞—á–µ CPU
    if torch.cuda.is_available():
        return 0
    return -1  # CPU

# ==== 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ ====

def read_docx_with_full_numbering(docx_path):
    ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}

    with ZipFile(docx_path) as z:
        doc_xml = etree.fromstring(z.read("word/document.xml"))
        styles_xml = etree.fromstring(z.read("word/styles.xml"))

    # --- 1Ô∏è‚É£ –°—Ç–∏–ª–∏ ‚Üí numId / ilvl ---
    style_map = {}
    for style in styles_xml.findall(".//w:style[@w:type='paragraph']", ns):
        style_id = style.get("{%s}styleId" % ns["w"])
        numPr = style.find(".//w:numPr", ns)
        if numPr is not None:
            numId = numPr.find("./w:numId", ns)
            ilvl = numPr.find("./w:ilvl", ns)
            style_map[style_id] = {
                "numId": int(numId.get("{%s}val" % ns["w"])) if numId is not None else None,
                "ilvl": int(ilvl.get("{%s}val" % ns["w"])) if ilvl is not None else 0,
            }

    # --- 2Ô∏è‚É£ –ê–±–∑–∞—Ü—ã + –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ ---
    paragraphs = []
    for p in doc_xml.findall(".//w:p", ns):
        text = "".join(t.text for t in p.findall(".//w:t", ns) if t.text)
        text = text.strip()
        styleEl = p.find(".//w:pStyle", ns)
        style_id = styleEl.get("{%s}val" % ns["w"]) if styleEl is not None else None
        numPr = p.find(".//w:numPr", ns)

        numId = ilvl = None
        if numPr is not None:
            numIdEl = numPr.find("./w:numId", ns)
            ilvlEl = numPr.find("./w:ilvl", ns)
            numId = int(numIdEl.get("{%s}val" % ns["w"])) if numIdEl is not None else None
            ilvl = int(ilvlEl.get("{%s}val" % ns["w"])) if ilvlEl is not None else 0
        elif style_id in style_map:
            numId = style_map[style_id]["numId"]
            ilvl = style_map[style_id]["ilvl"]

        paragraphs.append({
            "text": text,
            "numId": numId,
            "ilvl": ilvl if ilvl is not None else 0,
        })

    # --- 3Ô∏è‚É£ –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é ---
    counters = defaultdict(lambda: [0]*9)
    lines = []

    for p in paragraphs:
        numId = p["numId"]
        ilvl = p["ilvl"]
        text = p["text"]

        # –µ—Å–ª–∏ –Ω–æ–º–µ—Ä —É–∂–µ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º
        if re.match(r"^\s*\d+([-.]\s*\d+){0,2}([-.]\s*[A-Z–ê-–Ø])?\.", text):
            lines.append(text)
            continue

        # –µ—Å–ª–∏ –µ—Å—Ç—å –Ω—É–º–µ—Ä–∞—Ü–∏—è Word ‚Äî –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
        if numId is not None:
            counters[numId][ilvl] += 1
            for j in range(ilvl + 1, len(counters[numId])):
                counters[numId][j] = 0

            num_str = "-".join(str(x) for x in counters[numId][:ilvl + 1] if x > 0) + "."
            lines.append(f"{num_str} {text}")
        else:
            lines.append(text)

    return "\n".join(lines)

def upload_file(link):
    suffix = Path(link).suffix.lower()
    if suffix == '.docx':
        print('–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.')
        document_text = read_docx_with_full_numbering(link)
        return document_text

    elif suffix == '.pdf':
        with open(link, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            document_text = "\n<<<PAGE_BREAK>>>\n".join(
                page.extract_text() for page in reader.pages if page.extract_text()
            )
        print('–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.')
        return document_text

    else:
        return "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ docx –∏–ª–∏ pdf."

# ==== 2. –ü–∞—Ä—Å–µ—Ä —ç–ø–∏–∑–æ–¥–æ–≤ –∏ –ø–µ—Ä–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ ====

def parse_episode_from_text(scene_num, text, match_start):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–æ–º–µ—Ä —Å–µ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."""
    episode_num = None
    prev_text = text[:match_start]
    next_text = text[match_start:match_start + 200]  # –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–ª–µ —Ç–æ–∂–µ —Å–º–æ—Ç—Ä–∏–º

    # --- 1Ô∏è‚É£ –ü–æ —Å—Ü–µ–Ω–µ: 3-1, 3-2-A –∏ —Ç.–¥. ---
    if scene_num:
        ep_match = re.match(r"^(\d+)", scene_num)
        if ep_match:
            episode_num = ep_match.group(1)

    # --- 2Ô∏è‚É£ –¢–µ—Ö–Ω–æ-—Ñ–æ—Ä–º–∞—Ç —Ç–∏–ø–∞ –°1–≠03 –∏–ª–∏ –°02–ï05 (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è) ---
    pattern_tech = re.findall(
        r"[–°C]\s*(\d+)\s*[–≠E]\s*(\d+)",
        text,
        flags=re.IGNORECASE
    )
    if pattern_tech:
        season, episode = pattern_tech[-1]
        episode_num = episode

    # --- 2a) –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç: "1 –°–ï–ó–û–ù 1 –°–ï–†–ò–Ø" ---
    season_ep = re.findall(
        r"(\d+)\s*–°–ï–ó–û–ù[^\n]*?(\d+)\s*–°–ï–†–ò",
        text,
        flags=re.IGNORECASE
    )
    if season_ep:
        season, episode = season_ep[-1]
        episode_num = episode

    # --- 3Ô∏è‚É£ –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: ¬´—Ç—Ä–µ—Ç—å—è —Å–µ—Ä–∏—è¬ª, ¬´–∫–æ–Ω–µ—Ü —Ç—Ä–µ—Ç—å–µ–π —Å–µ—Ä–∏–∏¬ª ---
    # —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–µ—Ä–µ–¥ —Å—Ü–µ–Ω–æ–π
    word_match = re.search(
        r"(?:–ö–û–ù–ï–¶\s+)?([–ê-–Ø–∞-—è—ë\s-]+?)\s+–°–ï–†–ò–ò?",
        prev_text,
        flags=re.IGNORECASE
    )

    # –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî —Å–º–æ—Ç—Ä–∏–º –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å–ª–µ (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏—Å—Ç—ã —Å—Ç–∞–≤—è—Ç –ø–æ—Å–ª–µ —Å—Ü–µ–Ω)
    if not word_match:
        word_match = re.search(
            r"(?:–ö–û–ù–ï–¶\s+)?([–ê-–Ø–∞-—è—ë\s-]+?)\s+–°–ï–†–ò–ò?",
            next_text,
            flags=re.IGNORECASE
        )

    # –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –Ω–µ –Ω–∞—à–ª–∏ ‚Äî —Å–º–æ—Ç—Ä–∏–º "—Ö–≤–æ—Å—Ç" —Ç–µ–∫—Å—Ç–∞, –≥–¥–µ –æ–±—ã—á–Ω–æ "–ö–û–ù–ï–¶ ... –°–ï–†–ò–ò"
    if not word_match:
        tail_text = text[-2000:]
        word_match = re.search(
            r"(?:–ö–û–ù–ï–¶\s+)?([–ê-–Ø–∞-—è—ë0-9\s-]+?)\s+–°–ï–†–ò–ò?",
            tail_text,
            flags=re.IGNORECASE
        )

    raw_words = ""
    num_match = None
    if word_match:
        raw_words = word_match.group(1).strip()
        words_up = raw_words.upper()
        num_match = re.search(r"\b(\d{1,3})\b", words_up)

    if num_match:
        # –µ—Å–ª–∏ –≤ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ —è–≤–Ω–æ –µ—Å—Ç—å —Ü–∏—Ñ—Ä–∞ (–Ω–∞–ø—Ä. "1 –°–ï–†–ò–ò"), –±–µ—Ä—ë–º –µ—ë
        episode_num = num_match.group(1)
    else:
        # –∏–Ω–∞—á–µ –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å–ª–æ–≤–µ—Å–Ω–æ–µ —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ -> —á–∏—Å–ª–æ
        episode_num = russian_ordinal_to_int(raw_words.lower()) or episode_num

    return str(episode_num or "")


def russian_ordinal_to_int(phrase: str) -> int | None:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä—É—Å—Å–∫–∏–µ –ø–æ—Ä—è–¥–∫–æ–≤—ã–µ —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –≤ —á–∏—Å–ª–æ."""
    phrase = phrase.replace("-", " ").replace("–Å", "–ï").upper()
    ones = {
        "–ü–ï–†–í–ê–Ø": 1, "–í–¢–û–†–ê–Ø": 2, "–¢–†–ï–¢–¨–Ø": 3, "–ß–ï–¢–í–ï–†–¢–ê–Ø": 4, "–ü–Ø–¢–ê–Ø": 5,
        "–®–ï–°–¢–ê–Ø": 6, "–°–ï–î–¨–ú–ê–Ø": 7, "–í–û–°–¨–ú–ê–Ø": 8, "–î–ï–í–Ø–¢–ê–Ø": 9, "–î–ï–°–Ø–¢–ê–Ø": 10,
        "–û–î–ò–ù–ù–ê–î–¶–ê–¢–ê–Ø": 11, "–î–í–ï–ù–ê–î–¶–ê–¢–ê–Ø": 12, "–¢–†–ò–ù–ê–î–¶–ê–¢–ê–Ø": 13,
        "–ß–ï–¢–´–†–ù–ê–î–¶–ê–¢–ê–Ø": 14, "–ü–Ø–¢–ù–ê–î–¶–ê–¢–ê–Ø": 15, "–®–ï–°–¢–ù–ê–î–¶–ê–¢–ê–Ø": 16,
        "–°–ï–ú–ù–ê–î–¶–ê–¢–ê–Ø": 17, "–í–û–°–ï–ú–ù–ê–î–¶–ê–¢–ê–Ø": 18, "–î–ï–í–Ø–¢–ù–ê–î–¶–ê–¢–ê–Ø": 19,
        "–§–ò–ù–ê–õ–¨–ù–ê–Ø": 999
    }
    tens = {
        "–î–í–ê–î–¶–ê–¢–ê–Ø": 20, "–¢–†–ò–î–¶–ê–¢–ê–Ø": 30, "–°–û–†–û–ö–û–í–ê–Ø": 40, "–ü–Ø–¢–ò–î–ï–°–Ø–¢–ê–Ø": 50,
        "–®–ï–°–¢–ò–î–ï–°–Ø–¢–ê–Ø": 60, "–°–ï–ú–¨–î–ï–°–Ø–¢–ê–Ø": 70, "–í–û–°–¨–ú–ò–î–ï–°–Ø–¢–ê–Ø": 80,
        "–î–ï–í–Ø–ù–û–°–¢–ê–Ø": 90, "–°–¢–ê–Ø": 100
    }

    words = phrase.split()
    total = 0
    for word in words:
        if word in ones:
            total += ones[word]
        elif word in tens:
            total += tens[word]
        elif word.startswith("–î–í–ê–î–¶"): total += 20
        elif word.startswith("–¢–†–ò–î–¶"): total += 30
        elif word.startswith("–°–û–†–û–ö"): total += 40
        elif word.startswith("–ü–Ø–¢"): total += 5
        elif word.startswith("–®–ï–°–¢"): total += 6
        elif word.startswith("–°–ï–ú"): total += 7
        elif word.startswith("–í–û–°"): total += 8
        elif word.startswith("–î–ï–í"): total += 9
        elif word.startswith("–°–¢–û"): total += 100

    return total if total > 0 else None

def normalize_time(text: str) -> str:
    """
    –ò—â–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç:
    -> –ù–û–ß–¨ / –£–¢–†–û / –î–ï–ù–¨ / –í–ï–ß–ï–†
    """
    if not text:
        return ''
    
    t = text.lower().replace('—ë', '–µ')

    if re.search(r'\b(–Ω–æ—á—å—é|–Ω–æ—á—å|–ø–æ–∑–¥–Ω–æ –Ω–æ—á—å—é|–≥–ª—É–±–æ–∫–æ–π –Ω–æ—á—å—é)\b', t):
        return '–ù–û–ß–¨'
    if re.search(r'\b(—É—Ç—Ä–æ–º|—É—Ç—Ä–æ|–∫ —Ä–∞—Å—Å–≤–µ—Ç—É|–Ω–∞ —Ä–∞—Å—Å–≤–µ—Ç–µ|–ø–æ–¥ —É—Ç—Ä–æ|—Ä–∞—Å—Å–≤–µ—Ç(–µ|–∞|—É)|—Ä–∞—Å—Å–≤–µ—Ç)\b', t):
        return '–£–¢–†–û'
    if re.search(r'\b(–¥–Ω–µ–º|–¥–Ω–µ–º|–¥–µ–Ω—å|–≤ –ø–æ–ª–¥–µ–Ω—å|—Å–≤–µ—Ç–ª–æ)\b', t):
        return '–î–ï–ù–¨'
    if re.search(r'\b(–≤–µ—á–µ—Ä–æ–º|–≤–µ—á–µ—Ä|–ø–æ–∑–¥–Ω–∏–º –≤–µ—á–µ—Ä–æ–º|—Å—É–º–µ—Ä–∫–∏|–≤ —Å—É–º–µ—Ä–∫–∞—Ö|–∫ —Å—É–º–µ—Ä–∫–∞–º|–∏–∑ —Å—É–º–µ—Ä–µ–∫)\b', t):
        return '–í–ï–ß–ï–†'
    return ''

def parse_script_with_episode(pdf_text: str):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ü–µ–Ω–∞—Ä–∏–π –Ω–∞ —Å—Ü–µ–Ω—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç:
    - episode_num (–Ω–æ–º–µ—Ä —Å–µ—Ä–∏–∏)
    - scene_num (–Ω–æ–º–µ—Ä —Å—Ü–µ–Ω—ã)
    - location (–ò–ù–¢, –≠–ö–°–¢, –ù–ê–¢, –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏)
    - place (–º–µ—Å—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è, –æ—á–∏—â–µ–Ω–Ω–æ–µ)
    - time (–ù–û–ß–¨, –î–ï–ù–¨, –£–¢–†–û, –í–ï–ß–ï–† ‚Äî –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ fallback –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É)
    - text (—Ç–µ–ª–æ —Å—Ü–µ–Ω—ã)
    """

    # --- 0Ô∏è‚É£ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ---
    text = (pdf_text
    .replace('\xa0', ' ')
    .replace('‚Äì', '-')
    .replace('‚Äî', '-')
    .replace(' ', ' ')
    .replace('\r', '\n')
)

    # –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ —Å—Ü–µ–Ω–∞–º–∏, –ù–û –Ω–µ –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏ (—á—Ç–æ–±—ã 1.17 –Ω–µ –ª–æ–º–∞–ª–æ—Å—å)
    text = re.sub(r'(?<!\d)\.(?=\d{1,2}\s*[-.])', '.\n', text)
    text = re.sub(r'((\r?\n\s*){5,})', '\n<<<PAGE_BREAK_GAP>>>\n', text)

    # --- 1Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–æ–π —à–∞–±–ª–æ–Ω ---
    pattern = re.compile(r'''(?imx)
        ^
        \s*
        (?:–°–¶–ï–ù–ê\s*)?
        (?P<scene_num>
            \d+(?:[.-]\s*\d+){0,2}(?:[\s.-]*[A-Z–ê-–Ø0-9]{1,3})?
        )
        (?=\s*(?:\.|–ò–ù–¢|–≠–ö–°–¢|–ù–ê–¢|\n|<<<PAGE_BREAK))     # üëà –¥–æ–±–∞–≤–∏–ª–∏ –∑–∞—â–∏—Ç—É –æ—Ç —Å–∫–ª–µ–π–∫–∏ —á–µ—Ä–µ–∑ page break
        \.?\s*
        (?:–§–õ–ï–®–ë–ï–ö[^\n:.]*[:.]?|–§–õ–ï–®–ë–≠–ö[^\n:.]*[:.]?|FLASHBACK[^\n:.]*[:.]?)?\s*
        (?P<location>
            (?:–ò–ù–¢(?![–∞-—è])(?:\.|–ï–†–¨–ï–†)?|
            –≠–ö–°–¢(?![–∞-—è])(?:\.|–ï–†–¨–ï–†)?|
            –ù–ê–¢(?![–∞-—è])(?:\.|–£–†–ê)?)
            (?:\s*/\s*
                (?:–ò–ù–¢(?![–∞-—è])(?:\.|–ï–†–¨–ï–†)?|
                –≠–ö–°–¢(?![–∞-—è])(?:\.|–ï–†–¨–ï–†)?|
                –ù–ê–¢(?![–∞-—è])(?:\.|–£–†–ê)?)
            )?
        )
        [.\s:/-]*                                       # –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        (?P<place>                                      # –ª–æ–∫–∞—Ü–∏—è
            (?:[^\n\r]*?)
            (?=
                (?:–î–ï–ù–¨|–ù–û–ß–¨|–£–¢–†–û|–í–ï–ß–ï–†|–†–ê–°–°–í–ï–¢|–°–£–ú–ï–†–ö–ò)                # –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø–µ—Ä–µ–¥ –≤—Ä–µ–º–µ–Ω–µ–º
                |$
            )
        )
        [.\s:-]*
        (?P<time>(?:–î–ï–ù–¨|–ù–û–ß–¨|–£–¢–†–û|–í–ï–ß–ï–†|–†–ê–°–°–í–ï–¢|–°–£–ú–ï–†–ö–ò))?             # –≤—Ä–µ–º—è —Å—É—Ç–æ–∫ –±–µ–∑ –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤–∞
        [^\n]*\n?
        ''', re.IGNORECASE | re.MULTILINE | re.VERBOSE)


    # --- 2Ô∏è‚É£ –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ü–µ–Ω—ã ---
    matches = list(re.finditer(pattern, text))
    scenes = []
    current_episode = None  # –∞–≤—Ç–æ–Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Å–µ—Ä–∏–∏

    for i, match in enumerate(matches):
        start = match.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        block_text = text[start:end].strip()

        scene_num = (match.group("scene_num") or "").strip()
        location = (match.group("location") or "").strip().upper()
        raw_place = (match.group("place") or "").strip()
        time = (match.group("time") or "").strip().upper()
        scene_num = re.sub(r'\s+', '', (match.group("scene_num") or ""))
        scene_num = scene_num.strip('.-')

        raw_place = re.sub(r"\(\s*–°–ú\.[^)]*\)", "", raw_place, flags=re.IGNORECASE)
        raw_place = re.sub(r'[\s‚Äì:;.,-]+$', '', raw_place)
        place_norm = re.sub(r'\s+', ' ', raw_place).strip().upper()
        object_ = ""
        subobject = ""
        if place_norm:
            # —Ä–µ–∂–µ–º –ø–æ —Ç–æ—á–∫–µ, –Ω–æ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ –∫—É—Å–∫–∏
            parts = [p.strip() for p in place_norm.split('.') if p.strip()]
            if parts:
                object_ = parts[0]
                if len(parts) > 1:
                    # –≤—Å—ë, —á—Ç–æ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π "—Å–º—ã—Å–ª–æ–≤–æ–π" —á–∞—Å—Ç–∏, —Å—á–∏—Ç–∞–µ–º –ø–æ–¥–æ–±—ä–µ–∫—Ç–æ–º
                    subobject = ". ".join(parts[1:])

        scene_num = re.sub(r'\s+', '', (match.group("scene_num") or ""))
        scene_num = scene_num.strip('.-')

        # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ä–∏—é ---
        episode_num = parse_episode_from_text(scene_num, text, match.start())
        

        # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ä–µ–º—è —Å—É—Ç–æ–∫ ---
        if not time:  # –µ—Å–ª–∏ –Ω–µ –∏–∑–≤–ª–µ–∫–ª–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            snippet = block_text[:250]
            time = normalize_time(snippet)
        else:
            time = normalize_time(time)


        scenes.append({
            "episode_num": episode_num or "",
            "scene_num": scene_num,
            "location": location,
            "object": object_,      
            "subobject": subobject,   
            "time": time,
            "text": block_text,
        })

    return scenes

# ==== 3. –ü–∞—Ä—Å–µ—Ä —Å–ª–µ–¥—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫: –ø–µ—Ä—Å–æ–Ω–∞–∂–∏, –≥—Ä—É–ø–ø–æ–≤–∫–∞, –º–∞—Å—Å–æ–≤–∫–∞ ====

# -------------------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —à—Ç—É–∫–∏ --------------------

RUS_UP = "–ê-–Ø–Å"
RUS_LO = "–∞-—è—ë"

HEADER_TOKENS = {"–ò–ù–¢", "–ù–ê–¢", "–≠–ö–°–¢", "–î–ï–ù–¨", "–ù–û–ß–¨", "–£–¢–†–û", "–í–ï–ß–ï–†"}
SERV_MARKERS = {"–ó–ö", "–ó/–ö", "V.O.", "VO", "OFF"}
TRANSITION_MARKERS = {
    "cut", "cut to", "fade", "fade in", "fade out",
    "dissolve", "smash cut", "match cut", "super", "title", "slugline", "—Ç–∏—Ç—Ä", "—Å–ª–∞–≥–ª–∞–π–Ω", "–ø–µ—Ä–µ—Ö–æ–¥"
}
OFFSCREEN_LINE_MARKERS = {"–ì–ó–ö", "–ì–û–õ–û–° –ó–ê –ö–ê–î–†–û–ú"}


STOP_SINGLE = {"–≥—Ä–æ–º–∫–æ", "—à—É—Ç–ª–∏–≤–æ", "–≤—Å–µ–º", "–≤–≤–µ—Ä—Ö—É"}

ROLE_KEYWORDS = {
    "–æ—Ö—Ä–∞–Ω–Ω–∏–∫", "–æ—Ö—Ä–∞–Ω–Ω–∏—Ü–∞", "–∫–∞—Å—Å–∏—Ä", "–∫–∞—Å—Å–∏—Ä—à–∞", "–≤–æ–¥–∏—Ç–µ–ª—å", "—à–æ—Ñ—ë—Ä",
    "–≤—Ä–∞—á", "–¥–æ–∫—Ç–æ—Ä", "–º–µ–¥—Å–µ—Å—Ç—Ä–∞", "—Å–∞–Ω–∏—Ç–∞—Ä", "–∞–¥–≤–æ–∫–∞—Ç", "—é—Ä–∏—Å—Ç",
    "–æ—Ñ–∏—Ü–∏–∞–Ω—Ç", "–æ—Ñ–∏—Ü–∏–∞–Ω—Ç–∫–∞", "–º–µ–Ω–µ–¥–∂–µ—Ä", "—Å–µ–∫—Ä–µ—Ç–∞—Ä—å",
    "—Ç–∞–∫—Å–∏—Å—Ç", "—Ç–∞–∫—Å–∏—Å—Ç–∫–∞", "–ø–æ–ª–∏—Ü–µ–π—Å–∫–∏–π", "—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å",
    "–¥–∏—Ä–µ–∫—Ç–æ—Ä", "–Ω–∞—á–∞–ª—å–Ω–∏–∫", "–¥–µ–∫–∞–Ω", "–ø—Ä–æ–¥–∞–≤–µ—Ü", "–ø—Ä–æ–¥–∞–≤—â–∏—Ü–∞",
    "–æ–ø–µ—Ä–∞—Ç–æ—Ä", "–¥–µ–∂—É—Ä–Ω—ã–π", "–±–∞—Ä–º–µ–Ω", "–±–∞—Ä–º–µ–Ω—à–∞"
}

ROLE_SPEAKER_HEADS = {
    "–ß–ò–ù–û–í–ù–ò–ö",
    "–°–û–¢–†–£–î–ù–ò–ö",
    "–û–•–†–ê–ù–ù–ò–ö",
    "–°–õ–ï–î–û–í–ê–¢–ï–õ–¨",
    "–í–ï–î–£–©–ò–ô",
    "–°–£–î–¨–Ø",
    "–ê–î–í–û–ö–ê–¢"
}

# –ù–µ–±–æ–ª—å—à–æ–π —Å–ø–∏—Å–æ–∫ –º–∞—Ä–∫–µ—Ä–æ–≤ –¥–ª—è "–≥–æ–≤–æ—Ä—è—Ç –≤—Å–µ"
SPEECH_MODIFIERS = {"–ù–ê–ü–ï–†–ï–ë–û–ô", "–í–°–ï", "–í–°–Å", "–•–û–†–û–ú"}

PREPOSITIONS = {
    "–∫", "–≤", "–≤–æ", "–Ω–∞", "—É", "–æ", "–æ–±", "–æ–±–æ", "–æ—Ç",
    "–ø–æ", "—Å", "—Å–æ", "–∑–∞", "–¥–ª—è", "–∏–∑", "–∏–∑-–∑–∞", "–ø–æ–¥",
    "–Ω–∞–¥", "–ø—Ä–∏", "—á–µ—Ä–µ–∑", "–º–µ–∂–¥—É", "–ø–µ—Ä–µ–¥", "–ø—Ä–æ"
}

# === –ò–≥—Ä–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç: —Å–ª–æ–≤–∞—Ä—å –ª–µ–º–º ‚Üí –∫–∞–Ω–æ–Ω ===
TRANSPORT_MAP = {
    "–ú–ê–®–ò–ù–ê": {
        "–º–∞—à–∏–Ω–∞", "–∞–≤—Ç–æ–º–æ–±–∏–ª—å", "–∞–≤—Ç–æ", "—Ç–∞—á–∫–∞",
        "–ª–µ–≥–∫–æ–≤—É—à–∫–∞", "–¥–∂–∏–ø", "–≥—Ä—É–∑–æ–≤–∏–∫", "—Ñ—É—Ä–∞",
        "–º–∏–∫—Ä–æ–∞–≤—Ç–æ–±—É—Å", "—Ç–∞–∫—Å–æ–º–æ—Ç–æ—Ä"
    },
    "–ê–í–¢–û–ë–£–°": {"–∞–≤—Ç–æ–±—É—Å", "–º–∞—Ä—à—Ä—É—Ç–∫–∞", "–º–∞—Ä—à—Ä—É—Ç–Ω–æ–µ", "–ø–∞–∑–∏–∫"},
    "–ü–û–ï–ó–î": {"–ø–æ–µ–∑–¥", "—ç–ª–µ–∫—Ç—Ä–∏—á–∫–∞", "—Å–æ—Å—Ç–∞–≤", "–º–µ—Ç—Ä–æ–ø–æ–µ–∑–¥"},
    "–¢–†–ê–ú–í–ê–ô": {"—Ç—Ä–∞–º–≤–∞–π"},
    "–¢–†–û–õ–õ–ï–ô–ë–£–°": {"—Ç—Ä–æ–ª–ª–µ–π–±—É—Å"},
    "–ú–ï–¢–†–û": {"–º–µ—Ç—Ä–æ"},
    "–°–ê–ú–û–õ–Å–¢": {"—Å–∞–º–æ–ª–µ—Ç", "—Å–∞–º–æ–ª—ë—Ç", "–∞—ç—Ä–æ–ø–ª–∞–Ω", "–ª–∞–π–Ω–µ—Ä", "–±–æ—Ä—Ç"},
    "–í–ï–†–¢–û–õ–Å–¢": {"–≤–µ—Ä—Ç–æ–ª–µ—Ç", "–≤–µ—Ä—Ç–æ–ª—ë—Ç", "–≤–µ—Ä—Ç—É—à–∫–∞"},
    "–õ–û–î–ö–ê": {"–ª–æ–¥–∫–∞", "—à–ª—é–ø–∫–∞", "–±–∞–π–¥–∞—Ä–∫–∞", "–∫–∞–Ω–æ—ç"},
    "–ö–ê–¢–ï–†": {"–∫–∞—Ç–µ—Ä", "–∫–∞—Ç–µ—Ä–æ–∫"},
    "–Ø–•–¢–ê": {"—è—Ö—Ç–∞"},
    "–ö–û–†–ê–ë–õ–¨": {"–∫–æ—Ä–∞–±–ª—å", "—Å—É–¥–Ω–æ", "–ø–∞—Ä–æ—Ö–æ–¥", "–±–∞—Ä–∂–∞"},
    "–ú–û–¢–û–¶–ò–ö–õ": {"–º–æ—Ç–æ—Ü–∏–∫–ª", "–±–∞–π–∫", "—Ö–∞—Ä–ª–µ–π"},
    "–í–ï–õ–û–°–ò–ü–ï–î": {"–≤–µ–ª–æ—Å–∏–ø–µ–¥", "–±–∞–π–∫-–≤–µ–ª–æ—Å–∏–ø–µ–¥", "–≤–µ–ª–∏–∫"},
    "–ö–í–ê–î–†–û–¶–ò–ö–õ": {"–∫–≤–∞–¥—Ä–æ—Ü–∏–∫–ª"},
    "–°–ê–ù–ö–ò": {"—Å–∞–Ω–∏", "—Å–∞–Ω–∫–∏"},
    "–¢–ê–ö–°–ò": {"—Ç–∞–∫—Å–∏"},
    "–ü–õ–û–¢": {"–ø–ª–æ—Ç"}
}

IMPLICIT_MASS_GROUP_LEMMAS = {"—Ç–æ–ª–ø–∞", "–ª—é–¥–∏", "–ø—Ä–æ—Ö–æ–∂–∏–µ", "—Ç—É—Ä–∏—Å—Ç—ã", "–±–æ–ª–µ–ª—å—â–∏–∫–∏", "–∑—Ä–∏—Ç–µ–ª–∏", "–ø—É–±–ª–∏–∫–∞", "–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏", "–≥–æ—Å—Ç–∏"}

PUBLIC_LOCATION_LEMMAS = {"–≥–æ—Ä–æ–¥", "—É–ª–∏—Ü–∞", "–ø–ª–æ—â–∞–¥—å", "–ø–∞—Ä–∫", "—Å–∫–≤–µ—Ä", "–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è", "–º–µ—Ç—Ä–æ", "—Å—Ç–∞–Ω—Ü–∏—è", "–≤–æ–∫–∑–∞–ª", "–∞—ç—Ä–æ–ø–æ—Ä—Ç", "—Ç–æ—Ä–≥–æ–≤—ã–π", "—Ü–µ–Ω—Ç—Ä", "—Ä—ã–Ω–æ–∫", "—É–Ω–∏–≤–µ—Ä–º–∞–≥", "—Ç—Ü"}

SMALL_GROUP_NUM_WORDS = r"(–¥–≤–æ–µ|—Ç—Ä–æ–µ|—á–µ—Ç–≤–µ—Ä–æ|–ø—è—Ç–µ—Ä–æ|—à–µ—Å—Ç–µ—Ä–æ|—Å–µ–º–µ—Ä–æ|–≤–æ—Å—å–º–µ—Ä–æ|–¥–µ–≤—è—Ç–µ—Ä–æ|–¥–µ—Å—è—Ç–µ—Ä–æ)"
TEXT_GROUP_NOUNS = {"–æ—Ç—Ä—è–¥", "—É—á–∞—Å—Ç–Ω–∏–∫", "–∫–æ–º–∞–Ω–¥–∞", "–≥—Ä—É–ø–ø–∞", "—Ä–µ–±—è—Ç–∞"}
TEXT_GROUP_CANON = {
    "–æ—Ç—Ä—è–¥": "–û—Ç—Ä—è–¥",
    "—É—á–∞—Å—Ç–Ω–∏–∫": "–£—á–∞—Å—Ç–Ω–∏–∫–∏",
    "–∫–æ–º–∞–Ω–¥–∞": "–ö–æ–º–∞–Ω–¥–∞",
    "–≥—Ä—É–ø–ø–∞": "–ì—Ä—É–ø–ø–∞", 
    "—Ä–µ–±—è—Ç–∞": "–†–µ–±—è—Ç–∞"
}

GRIM_NOUN_LEMMAS = {"–≥—Ä–∏–º", "–º–∞–∫–∏—è–∂", "—Ç–æ–Ω–∞–ª—å–Ω–∏–∫", "—Ç–æ–Ω–∞–ª—å–Ω—ã–π", "–ø—É–¥—Ä–∞", "—Ä—É–º—è–Ω–∞", "—Ç—É—à—å", "–ø–æ–º–∞–¥–∞", "–ø–æ–º–∞–¥–∫–∞", "–ø–æ–¥–≤–æ–¥–∫–∞", "—Ä–µ—Å–Ω–∏—Ü—ã", "–±–æ—Ä–æ–¥–∞", "–±–∏–Ω—Ç",
"—É—Å—ã", "–ø–∞—Ä–∏–∫", "—à—Ä–∞–º", "—à—Ä–∞–º—ã", "—Å–∏–Ω—è–∫", "—Å–∏–Ω—è–∫–∏", "—Å—Å–∞–¥–∏–Ω–∞", "—Ü–∞—Ä–∞–ø–∏–Ω–∞", "—Ä–∞–Ω–∞", "—à—Ä–∞–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∫—Ä–æ–≤—å", "–∫—Ä–æ–≤–∏—â–∞", "–≥—Ä—è–∑—å", "–ø–ª–∞—Å—Ç—ã—Ä—å", "—Ç–∞—Ç—É", "—Ç–∞—Ç—É—Ö–∞", "—Ç–∞—Ç—É–∏—Ä–æ–≤–∫–∞"}

GRIM_ADJ_LEMMAS = {"–∑–∞–º–∞–∑–∞–Ω–Ω—ã–π", "–∏–∑–±–∏—Ç—ã–π", "–ø–æ–¥–±–∏—Ç—ã–π", "–∫—Ä–æ–≤–∞–≤—ã–π", "–∫—Ä–æ–≤–æ—Ç–æ—á–∞—â–∏–π", "—Å–∏–Ω—é—à–Ω—ã–π", "—Å–∏–Ω–µ–≤–∞—Ç—ã–π", "–≥—Ä—è–∑–Ω—ã–π", "—Ä–∞—Å–∫—Ä–∞—à–µ–Ω–Ω—ã–π", "–∑–∞–≥—Ä–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π", "–æ–∫—Ä—É–≥–ª–∏–≤—à–∏–π—Å—è",
                   "–ø–æ—Ö—É–¥–µ–≤—à–∏–π", "–≤–ø–∞–ª—ã–π"}

GRIM_NOUN_LEMMAS_NORM = {
    w.replace("—ë", "–µ") for w in GRIM_NOUN_LEMMAS
}
GRIM_ADJ_LEMMAS_NORM = {
    w.replace("—ë", "–µ") for w in GRIM_ADJ_LEMMAS
}

# === –ö–û–°–¢–Æ–ú / –û–î–ï–ñ–î–ê ===

COSTUME_NOUN_LEMMAS = {
    "–∫–æ—Å—Ç—é–º", "–∫–∏—Ç–µ–ª—å",
    "—Ñ–æ—Ä–º–∞", "—É–Ω–∏—Ñ–æ—Ä–º–∞",
    "—Ñ—É—Ä–∞–∂–∫–∞", "–∫–µ–ø–∫–∞", "—à–∞–ø–∫–∞", "–∫–∞–ø—é—à–æ–Ω",
    "—Ä—É–±–∞—à–∫–∞", "–±–ª—É–∑–∫–∞", "—Ñ—É—Ç–±–æ–ª–∫–∞", "–º–∞–π–∫–∞",
    "—Å–≤–∏—Ç–µ—Ä", "–∫–æ—Ñ—Ç–∞", "—Ç–æ–ª—Å—Ç–æ–≤–∫–∞", "—Ö—É–¥–∏",
    "–ø–∞–ª—å—Ç–æ", "–∫—É—Ä—Ç–∫–∞", "–ø–ª–∞—â", "–ø–∏–¥–∂–∞–∫", "–∂–∏–ª–µ—Ç",
    "–∫–æ–º–±–∏–Ω–µ–∑–æ–Ω", "—Å–ø–µ—Ü–æ–≤–∫–∞", "—Ö–∞–ª–∞—Ç",
    "–ø–ª–∞—Ç—å–µ", "—é–±–∫–∞",
    "–¥–∂–∏–Ω—Å—ã", "—à—Ç–∞–Ω—ã", "–±—Ä—é–∫–∏",
    "–±–æ—Ç–∏–Ω–∫–∏", "–∫—Ä–æ—Å—Å–æ–≤–∫–∏", "—Ç—É—Ñ–ª–∏", "–±–µ—Ä—Ü—ã", "—Å–∞–ø–æ–≥–∏",
    "–≥–∞–ª—Å—Ç—É–∫", "–±–∞–±–æ—á–∫–∞",
    "–º–∞—Å–∫–∞", "—Ç—Ä—É—Å—ã", "—Å—Ç—Ä–∏–Ω–≥–∏", "—Å—Ç—Ä–∏–Ω–≥", "–æ–¥–µ–∂–¥–∞"
}

COSTUME_ADJ_LEMMAS = {
    "—à–∫–æ–ª—å–Ω—ã–π",
    "–≤–æ–µ–Ω–Ω—ã–π",
    "–ø–∞—Ä–∞–¥–Ω—ã–π",
    "—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π",
    "–¥–µ–ª–æ–≤–æ–π",
    "—Ä–∞–±–æ—á–∏–π",
    "—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π",
    "–ø–æ—Ö–æ–¥–Ω—ã–π",
    "—Å–ø–∞—Å–∞—Ç–µ–ª—å–Ω—ã–π"
}

COSTUME_NOUN_ROOTS = {
    "—Ñ–æ—Ä–º",      # —Ñ–æ—Ä–º–∞, —Ñ–æ—Ä–º–µ–Ω–Ω–æ–π, –≤ —Ñ–æ—Ä–º–µ‚Ä¶
    "—É–Ω–∏—Ñ–æ—Ä–º",
}

# –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º "–µ/—ë"
COSTUME_NOUN_LEMMAS_NORM = {
    w.replace("—ë", "–µ") for w in COSTUME_NOUN_LEMMAS
}
COSTUME_ADJ_LEMMAS_NORM = {
    w.replace("—ë", "–µ") for w in COSTUME_ADJ_LEMMAS
}

# === —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –î–µ–∫–æ—Ä–∞—Ü–∏—è / –ü–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞ / –ö–∞—Å–∫–∞–¥–µ—Ä / –°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç ===

# –î–ï–ö–û–†–ê–¶–ò–Ø: —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞, –∞ –Ω–µ –∫ –º–µ–ª–∫–æ–º—É —Ä–µ–∫–≤–∏–∑–∏—Ç—É
# –ª–æ–∫–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å—á–∏—Ç–∞–µ–º "–ø—Ä–∏—Ä–æ–¥–æ–π" (–Ω–µ –¥–µ–∫–æ—Ä–∞—Ü–∏—è)
NATURAL_PLACES = {
    "–≥–æ—Ä—ã", "–≥–æ—Ä–∞", "–ª–µ—Å", "–ø–æ–ª–µ", "–ª—É–≥", "—Å—Ç–µ–ø—å",
    "—Ä–µ–∫–∞", "–æ–∑–µ—Ä–æ", "–º–æ—Ä–µ", "–ø–ª—è–∂", "–ø—É—Å—Ç—ã–Ω—è",
    "—Å–µ—Ä–ø–∞–Ω—Ç–∏–Ω", "–¥–æ—Ä–æ–≥–∞", "—Ç—Ä–∞—Å—Å–∞", "—É–ª–∏—Ü–∞", "–≥–æ—Ä–æ–¥"
}

# —è–≤–Ω–æ —Ä—É–∫–æ—Ç–≤–æ—Ä–Ω—ã–µ –º–µ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏–º —Å—á–∏—Ç–∞—Ç—å –¥–µ–∫–æ—Ä–∞—Ü–∏–µ–π,
# –¥–∞–∂–µ –µ—Å–ª–∏ —Å—Ü–µ–Ω–∞ –ù–ê–¢
MANMADE_PLACES = {
    "–ª–∞–≥–µ—Ä—å", "–ø–∞–ª–∞—Ç–æ—á–Ω—ã–π –ª–∞–≥–µ—Ä—å", "–±–∞–∑–∞", "—Å—Ç–∞–Ω—Ü–∏—è",
    "–ª–æ–¥–æ—á–Ω–∞—è —Å—Ç–∞–Ω—Ü–∏—è", "–ª–æ–¥–æ—á–Ω–∞—è", "–ø—Ä–∏—Å—Ç–∞–Ω—å", "–ø—Ä–∏—Å—Ç–∞–Ω–∏",
    "–ø–æ—Ä—Ç", "–≤–æ–∫–∑–∞–ª", "—Å—Ç–∞–Ω—Ü–∏—è –º–µ—Ç—Ä–æ", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞",
    "—à–∫–æ–ª–∞", "–±–æ–ª—å–Ω–∏—Ü–∞", "—Å—É–¥", "–∫–ª—É–±", "–±–∞—Ä", "–∫–∞—Ñ–µ",
    "—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "–º–∞–≥–∞–∑–∏–Ω", "—Ä—ã–Ω–æ–∫", "–≥–æ—Å—Ç–∏–Ω–∏—Ü–∞", "–æ—Ç–µ–ª—å",
    "–¥–≤–æ—Ä", "–ø–æ–¥—ä–µ–∑–¥", "–ø–æ–¥–∑–µ–º–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥", "–ø–æ–¥–∑–µ–º–∫–∞"
}

# –æ–±—â–∏–µ —Å—Ç–æ–ø—ã —É—Ä–æ–≤–Ω—è "–ì–û–†–û–î", "–£–õ–ò–¶–ê", "–ì–û–†–´" –∏ —Ç.–ø. ‚Äî
# –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç *—Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ* –∏–∑ —Ç–∞–∫–æ–≥–æ —Å–ª–æ–≤–∞, –¥–µ–∫–æ—Ä–∞—Ü–∏–µ–π –Ω–µ —Å—á–∏—Ç–∞–µ–º
GENERIC_PLACE_STOP = NATURAL_PLACES | {
    "–≥–æ—Ä–æ–¥", "—É–ª–∏—Ü–∞", "–º–µ—Å—Ç–æ", "–ø–ª–æ—â–∞–¥–∫–∞", "—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è"
}



# –ü–ò–†–û–¢–ï–•–ù–ò–ö–ê
PYRO_NOUNS = {
    "–≤–∑—Ä—ã–≤", "–≤–∑—Ä—ã–≤—ã", "–≤–∑—Ä—ã–≤—á–∞—Ç–∫–∞",
    "—Ñ–µ–π–µ—Ä–≤–µ—Ä–∫", "—Å–∞–ª—é—Ç", "–ø–µ—Ç–∞—Ä–¥–∞", "–ø–µ—Ç–∞—Ä–¥—ã",
    "—Ä–∞–∫–µ—Ç–∞", "—Ä–∞–∫–µ—Ç—ã", "–ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–∑–∞–ª–ø", "–æ—Å–∫–æ–ª–∫–∏",
    "–æ–≥–Ω–µ–Ω–Ω—ã–π —à–∞—Ä", "–æ–≥–Ω–µ–Ω–Ω—ã–π —Å—Ç–æ–ª–±", "–∫–æ—Å—Ç–µ—Ä"
}
PYRO_VERBS = {
    "–≤–∑—Ä—ã–≤–∞—Ç—å—Å—è", "–≤–∑–æ—Ä–≤–∞—Ç—å—Å—è", "–ø–æ–¥—Ä—ã–≤–∞—Ç—å", "–¥–µ—Ç–æ–Ω–∏—Ä–æ–≤–∞—Ç—å",
    "—Å—Ç—Ä–µ–ª—è—Ç—å", "–≤—ã—Å—Ç—Ä–µ–ª–∏—Ç—å", "—Ä–≤–∞–Ω—É—Ç—å", "–ø–æ–ª—ã—Ö–Ω—É—Ç—å", "–ø–æ–ª—ã—Ö–∞—Ç—å"
}

# –ö–ê–°–ö–ê–î–ï–†
STUNT_WORDS = {
    "–∫–∞—Å–∫–∞–¥–µ—Ä", "–∫–∞—Å–∫–∞–¥–µ—Ä—ã", "–∫–∞—Å–∫–∞–¥–µ—Ä—Å–∫–∏–π",
    "–¥—É–±–ª–µ—Ä", "–¥—É–±–ª–µ—Ä—ã", "–¥—É–±–ª—ë—Ä", "–¥—É–±–ª—ë—Ä—ã",
    "—Ç—Ä—é–∫–∞—á", "—Ç—Ä—é–∫–∞—á–∏"
}

# –°–ü–ï–¶–≠–§–§–ï–ö–¢–´
FX_NOUNS = {
    "—Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç", "—Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã", "—ç—Ñ—Ñ–µ–∫—Ç", "—ç—Ñ—Ñ–µ–∫—Ç—ã",
    "–¥—ã–º–æ–≤–∞—è –∑–∞–≤–µ—Å–∞", "–¥—ã–º", "—Ç—É–º–∞–Ω", "–∏—Å–∫—Ä–∞", "–∏—Å–∫—Ä—ã", "–¥—ã–º–∫–∞",
    "cg", "cgi", "vfx", "–∞–Ω–∏–º–∞—Ü–∏—è", "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞",
    "slowmotion", "—Å–ª–æ—É-–º–æ", "—Å–ª–æ—É–º–æ", "–∑–∞–º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—ä–µ–º–∫–∞", "—Ñ–ª–µ—à–±–µ–∫", "—Ñ–ª—ç—à–±–µ–∫", "—Ñ–ª–µ—à–±—ç–∫", "flashback"
}
FX_KEYWORDS = {
    "–∑–∞–º–µ–¥–ª–µ–Ω–Ω–æ–π —Å—ä–µ–º–∫–µ", "–∑–∞–º–µ–¥–ª–µ–Ω–Ω–æ–π —Å—ä—ë–º–∫–µ",
    "–∑–∞–º–µ–¥–ª–µ–Ω–Ω–æ", "—Å–ª–æ—É-–º–æ", "slow motion", "slow-motion",
    "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫–æ–π", "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞",
}

PYRO_NOUNS_N  = {w.replace("—ë", "–µ") for w in PYRO_NOUNS}
PYRO_VERBS_N  = {w.replace("—ë", "–µ") for w in PYRO_VERBS}

STUNT_WORDS_N = {w.replace("—ë", "–µ") for w in STUNT_WORDS}

FX_NOUNS_N    = {w.replace("—ë", "–µ") for w in FX_NOUNS}
FX_KEYWORDS_N = {w.replace("—ë", "–µ") for w in FX_KEYWORDS}


def load_ru():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—é–±—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é ru-–º–æ–¥–µ–ª—å spaCy.
    """
    #for name in ("ru_core_news_lg", "ru_core_news_md", "ru_core_news_sm"):
    for name in ("ru_core_news_sm"):
        try:
            #return spacy.load(name)
            nlp = ru_core_news_sm.load()
            return nlp
        except OSError:
            continue
    raise RuntimeError(
        "–ù–µ –Ω–∞–π–¥–µ–Ω ru_core_news_*. –£—Å—Ç–∞–Ω–æ–≤–∏ –º–æ–¥–µ–ª—å, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n"
        "python -m spacy download ru_core_news_sm"
    )

nlp = load_ru()

def _norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

#PYMORPHY_DICT_DIR = BASE_DIR / "pymorphy3_dicts_ru"
PYMORPHY_DICT_DIR = resource_path("pymorphy3_dicts_ru")
morph = MorphAnalyzer()
#morph = MorphAnalyzer(dict_path=str(PYMORPHY_DICT_DIR))

def ru_lemma(token_text: str) -> str:
    """
    –ë–µ—Ä—ë–º –ª–µ–º–º—É —á–µ—Ä–µ–∑ pymorphy3.
    –ï—Å–ª–∏ –≤–¥—Ä—É–≥ pymorphy –Ω–µ —Å–ø—Ä–∞–≤–∏–ª—Å—è ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º lowercase.
    """
    p = morph.parse(token_text)
    if not p:
        return token_text.lower()
    return p[0].normal_form.lower()

def _strip_punct_tail(s: str) -> str:
    return re.sub(r"[!?‚Ä¶\.:,;]+$", "", s).strip()


def _is_caps_line(s: str) -> bool:
    """
    –°—Ç—Ä–æ–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –ö–ê–ü–° (–¥–∏–∞–ª–æ–≥–æ–≤—ã–π –º–∞—Ä–∫–µ—Ä).
    """
    s = s.strip()
    if not s:
        return False
    # —Ü–µ–ª–∏–∫–æ–º –≤ —Å–∫–æ–±–∫–∞—Ö ‚Äî —Ä–µ–º–∞—Ä–∫–∞, –∞ –Ω–µ —Å–ø–∏–∫–µ—Ä
    if s.startswith("(") and s.endswith(")"):
        return False
    letters = re.sub(r"[^A-Za-z–ê-–Ø–∞-—è–Å—ë]", "", s)
    if not letters:
        return False
    return letters == letters.upper() and len(letters) >= 2


def _looks_like_header(s: str) -> bool:
    """
    –ü–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ —Å—Ç—Ä–æ–∫—É-–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ü–µ–Ω—ã.
    """
    S = s.strip().upper()
    return (
        bool(re.search(r"\d+\s*[-‚Äì.]", S))
        or any(k in S for k in HEADER_TOKENS)
        or S.count(".") >= 2
    )


def _clean_caps_name(line: str) -> str:
    """
    –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–∫–æ–±–∫–∏ –∏–∑ –ö–ê–ü–°-—Å—Ç—Ä–æ–∫–∏:
    '–°–¢–≠–õ–õ–ê (–ó–ö)' -> '–°–¢–≠–õ–õ–ê'
    """
    s = re.sub(r"\s*\(.*?\)\s*", "", line)
    s = s.strip(" .:-")
    return s


def _is_service_caps_line(s: str) -> bool:
    """
    –ö–ê–ü–°-—Å—Ç—Ä–æ–∫–∞ —Å –º–æ–Ω—Ç–∞–∂–Ω—ã–º/—Å–ª—É–∂–µ–±–Ω—ã–º –º–∞—Ä–∫–µ—Ä–æ–º (–Ω–µ —Å–ø–∏–∫–µ—Ä).
    """
    if not _is_caps_line(s):
        return False
    us = s.strip().upper().strip(" .:-")
    if any(marker in us for marker in OFFSCREEN_LINE_MARKERS):
        return True
    # –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å –ø–æ –º–µ—Ä–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º—É—Å–æ—Ä–∞
    if us in HEADER_TOKENS or us in SERV_MARKERS:
        return True
    return False


def _clean_person_name(name: str) -> str:
    """
    –ü–æ–¥—á–∏—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–∏—Ä–µ, –ø—Ä–æ–±–µ–ª—ã –∏ —Ö–≤–æ—Å—Ç–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é.
    '–ê—Ä–∏–Ω–µ -' -> '–ê—Ä–∏–Ω–µ', '–ú–∞—Å—Å–µ...' -> '–ú–∞—Å—Å–µ'
    """
    name = re.sub(r"\s*[-‚Äì‚Äî]+\s*$", "", name)
    name = _strip_punct_tail(name)
    return _norm(name)

def _has_digit(s: str) -> bool:
    return any(ch.isdigit() for ch in s)

# –†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –¥–ª—è —Ä–µ–∫–≤–∏–∑–∏—Ç–∞
ALLOWED_REKV_POS = {"NOUN", "PROPN", "ADJ"}

# –ü—Ä–æ—Å—Ç–µ–π—à–∏–π —Ñ–∏–ª—å—Ç—Ä —à—É–º–∞ ‚Äî –ø–æ—Ç–æ–º —Å–º–æ–∂–µ—à—å –ø–æ–ø–æ–ª–Ω–∏—Ç—å
REKV_NOISE_PAT = re.compile(r"^[–ê-–Ø–∞-—è–Å—ë]+$")  # —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞, –±–µ–∑ —Ü–∏—Ñ—Ä –∏ –∑–Ω–∞–∫–æ–≤


def _basic_rekv_filter(text: str, min_len: int = 2) -> bool:
    """
    –ì—Ä—É–±—ã–π —Ñ–∏–ª—å—Ç—Ä: –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —è–≤–Ω–æ –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç *–ø—Ä–æ—Ö–æ–¥–∏—Ç* —Ñ–∏–ª—å—Ç—Ä.
    """
    s = text.strip()
    if len(s) < min_len:
        return False
    # –µ—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ—Ç –±—É–∫–≤
    if not re.search(r"[–ê-–Ø–∞-—è–Å—ë]", s):
        return False
    # –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º "—Å—Ç—Ä–∞–Ω–Ω—ã–µ" —Å–∏–º–≤–æ–ª—ã (–æ—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã/–ø—Ä–æ–±–µ–ª/–¥–µ—Ñ–∏—Å/–∫–∞–≤—ã—á–∫–∏)
    if re.search(r"[^–ê-–Ø–∞-—è–Å—ë \-¬´¬ª\"']", s):
        return False
    return True


ALLOWED_REKV_POS = {"ADJ", "NOUN", "PROPN"}

def normalize_phrase_adj_noun(phrase: str, nlp) -> str:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä '–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ + —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ' 
    –¥–ª—è —Ä–µ–∫–≤–∏–∑–∏—Ç–∞/–∫–æ—Å—Ç—é–º–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –û–î–ù–£ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ñ—Ä–∞–∑—É (–∫–ª—é—á –∫–ª–∞—Å—Ç–µ—Ä–∞).
    """
    doc = nlp(phrase)
    tokens = [t for t in doc if t.is_alpha and t.pos_ in ALLOWED_REKV_POS]
    if not tokens:
        return ""

    # (lemma, POS) —Å pymorphy3 –¥–ª—è –ª–µ–º–º
    lemmas_pos = [(ru_lemma(t.text), t.pos_) for t in tokens]

    phrases = []
    i = 0
    while i < len(lemmas_pos):
        lem, pos = lemmas_pos[i]

        # ADJ + NOUN/PROPN
        if pos == "ADJ" and i + 1 < len(lemmas_pos) and lemmas_pos[i + 1][1] in {"NOUN", "PROPN"}:
            lem2, _ = lemmas_pos[i + 1]
            phrases.append(f"{lem} {lem2}")   # "–ø–æ—Ö–æ–¥–Ω—ã–π –æ–¥–µ–∂–¥–∞"
            i += 2
        elif pos in {"NOUN", "PROPN"}:
            phrases.append(lem)                # –æ–¥–∏–Ω–æ—á–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
            i += 1
        else:
            i += 1

    if not phrases:
        return ""

    norm = phrases[0].replace("—ë", "–µ")
    return norm[:1].upper() + norm[1:]



def clean_requisite_entities(raw_ents, nlp, min_score: float = 0.4):
    """
    raw_ents: [{"entity","word","score"}, ...] –¥–ª—è –†–ï–ö–í–ò–ó–ò–¢.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫:
      {
        "lemma":   "–§–æ–Ω–∞—Ä–∏–∫",            # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–∞–Ω–æ–Ω
        "surface": ["–§–æ–Ω–∞—Ä–∏–∫–∞", ...],    # –∏—Å—Ö–æ–¥–Ω—ã–µ —Ñ–æ—Ä–º—ã
        "score":   0.849                 # –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score –≤ –≥—Ä—É–ø–ø–µ
      }
    """
    buckets = []
    vowels = set("–∞–µ—ë–∏–æ—É—ã—ç—é—è")

    for ent in raw_ents:
        score = float(ent.get("score", 0.0))
        if score < min_score:
            continue

        phrase = (ent.get("word") or "").strip()
        if len(phrase) < 2:
            continue

        if not re.search(r"[–ê-–Ø–∞-—è–Å—ë]", phrase):
            continue

        # --- 1) –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–ª—é—á —á–µ—Ä–µ–∑ —Ç–≤–æ—é —Ñ—É–Ω–∫—Ü–∏—é ---
        norm = normalize_phrase_adj_noun(phrase, nlp)   # ‚Üê –≤–æ—Ç —Ç—É—Ç "–§–æ–Ω–∞—Ä–∏–∫–∞" –¥–æ–ª–∂–Ω–∞ —Å—Ç–∞—Ç—å "–§–æ–Ω–∞—Ä–∏–∫"
        if not norm:
            continue

        norm_low = norm.lower()
        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–æ–≤—Å–µ–º —Å—Ç—Ä–∞–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π
        if len(norm_low) < 3 or not any(ch in vowels for ch in norm_low):
            continue

        buckets.append(
            {
                "lemma": norm,        # "–§–æ–Ω–∞—Ä–∏–∫"
                "surface": phrase,    # "–§–æ–Ω–∞—Ä–∏–∫–∞" / "–§–æ–Ω–∞—Ä–∏–∫"
                "score": score,
            }
        )

    if not buckets:
        return []

    # 2) —Å–∫–ª–µ–π–∫–∞ –ø–æ –±–∞–∑–µ –ª–µ–º–º—ã (–µ—Å–ª–∏ —Ö–æ—á–µ—à—å –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –§–û–ù–ê–†–¨/–§–û–ù–ê–†–ò–ö)
    cluster_map = defaultdict(list)
    for item in buckets:
        base = _lemma_base(item["lemma"].lower())   # –Ω–∞–ø—Ä–∏–º–µ—Ä, "—Ñ–æ–Ω–∞—Ä"
        cluster_map[base].append(item)

    cleaned = []
    for base, items in cluster_map.items():
        # –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é —Ñ–æ—Ä–º—É –ø–æ score
        best = max(items, key=lambda x: x["score"])
        canon = best["lemma"]     # —É–∂–µ "–§–æ–Ω–∞—Ä–∏–∫"

        surfaces = []
        scores   = []
        for it in items:
            if it["surface"] not in surfaces:
                surfaces.append(it["surface"])
            scores.append(it["score"])

        cleaned.append(
            {
                "lemma": canon,                # —á—Ç–æ —É–π–¥—ë—Ç –≤ —Ç–∞–±–ª–∏—Ü—É
                "surface": surfaces,           # –∫–∞–∫–∏–µ —Ñ–æ—Ä–º—ã –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å
                "score": round(max(scores), 3),
            }
        )

    cleaned.sort(key=lambda x: -x["score"])
    return cleaned


# -------------------- –î–∞–Ω–Ω—ã–µ: —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ --------------------

@dataclass
class Mention:
    text: str                  # —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    kind: str                  # 'dialog', 'header', 'ner'
    span: Tuple[int, int]      # (start_char, end_char) –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å—Ü–µ–Ω—ã
    line_idx: int              # –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏ (0-based)
    lemmas: Tuple[str, ...]    # –ª–µ–º–º—ã spaCy (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)
    is_anchor: bool = False    # –¥–∏–∞–ª–æ–≥/—à–∞–ø–∫–∞ ‚Üí —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª


@dataclass
class Character:
    id: int
    canonical_name: str
    aliases: Set[str] = field(default_factory=set)
    is_main: bool = True               # –µ—Å—Ç—å –ª–∏ –¥–∏–∞–ª–æ–≥–æ–≤–æ–µ –∏–º—è
    source: str = ""                   # 'dialog', 'header', 'mixed', ...
    confidence: float = 1.0


# -------------------- –®–∞–≥ 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∏–º—ë–Ω --------------------

def extract_dialog_speakers(
    text: str,
    nlp,
) -> List[Tuple[str, int, int, int]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ö–ê–ü–°-—Å—Ç—Ä–æ–∫–∏-—Å–ø–∏–∫–µ—Ä—ã.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (name, line_idx, start_char, end_char).

    –°—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–º, –µ—Å–ª–∏:
      - —ç—Ç–æ –ö–ê–ü–°, –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –Ω–µ —Å–µ—Ä–≤–∏—Å,
      - –≤ —Å—Ç—Ä–æ–∫–µ –Ω–µ—Ç .?!‚Ä¶,:,
      - –∏ –õ–ò–ë–û –µ—Å—Ç—å PROPN –≤ –∞–Ω–∞–ª–∏–∑–µ —ç—Ç–æ–π —Å—Ç—Ä–æ–∫–∏,
        –õ–ò–ë–û –µ—ë —Ç–æ–∫–µ–Ω—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ NER-PER –ø–æ –≤—Å–µ–π —Å—Ü–µ–Ω–µ,
        –õ–ò–ë–û –µ—ë —Ç–æ–∫–µ–Ω—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Å—Ü–µ–Ω–µ –±–æ–ª—å—à–µ 1 —Ä–∞–∑–∞.
    """

    # –ø—Ä–æ–≥–æ–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –æ–¥–∏–Ω —Ä–∞–∑
    doc_full = nlp(text)

    # —á–∞—Å—Ç–æ—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å—Ü–µ–Ω–µ
    token_freq: Dict[str, int] = {}
    for tok in doc_full:
        if tok.is_alpha:
            key = tok.text.lower()
            token_freq[key] = token_freq.get(key, 0) + 1

    # –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–º—ë–Ω –∏–∑ NER (PER)
    per_tokens: Set[str] = set()
    for ent in doc_full.ents:
        if ent.label_ != "PER":
            continue
        for t in ent:
            if t.is_alpha:
                per_tokens.add(t.text.lower())

    speakers: List[Tuple[str, int, int, int]] = []

    lines = text.splitlines(keepends=True)
    offset = 0
    for i, raw in enumerate(lines):
        line = raw.rstrip("\n")

        # 1. –ö–ê–ü–° + –Ω–µ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ü–µ–Ω—ã
        if not _is_caps_line(line) or _looks_like_header(line):
            offset += len(raw)
            continue

        # 2. —Å–µ—Ä–≤–∏—Å/–º–æ–Ω—Ç–∞–∂
        if _is_service_caps_line(line):
            offset += len(raw)
            continue

        cleaned = _clean_caps_name(line)
        if not cleaned:
            offset += len(raw)
            continue

        # üîπ –æ—Ç—Ä–µ–∑–∞–µ–º –≤—Å—ë, —á—Ç–æ –≤ –∫—Ä—É–≥–ª—ã—Ö —Å–∫–æ–±–∫–∞—Ö: "–¢–ò–ú–£–† (–ó/–ö, –®–£–¢–õ–ò–í–û)" ‚Üí "–¢–ò–ú–£–†"
        cleaned = re.sub(r"\(.*", "", cleaned).strip()
        if not cleaned:
            offset += len(raw)
            continue

        up = cleaned.upper()
        lo = cleaned.lower()

        # 3. –µ—Å–ª–∏ –µ—Å—Ç—å .?!‚Ä¶,: ‚Äî —ç—Ç–æ —Ç–µ–∫—Å—Ç —Ä–µ–ø–ª–∏–∫–∏, –∞ –Ω–µ –∏–º—è
        if any(ch in cleaned for ch in ".!?‚Ä¶,:"):
            offset += len(raw)
            continue

        # 4. —Å—Ç—Ä–æ–∫–∏ —Å –∑–∞–ø—è—Ç—ã–º–∏ ‚Äî —ç—Ç–æ —à–∞–ø–∫–∏ —Å–æ —Å–ø–∏—Å–∫–∞–º–∏ –∏–º—ë–Ω, —Ä–∞–∑–±–µ—Ä—ë–º –æ—Ç–¥–µ–ª—å–Ω–æ
        if "," in cleaned:
            offset += len(raw)
            continue

        # 5. –º–æ–Ω—Ç–∞–∂–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
        if lo in TRANSITION_MARKERS or up in HEADER_TOKENS or up in SERV_MARKERS:
            offset += len(raw)
            continue

        parts = [p for p in re.split(r"[,\s]+", cleaned) if p]
        if len(parts) >= 2 and parts[-1].upper() in SPEECH_MODIFIERS:
            offset += len(raw)
            continue

        # --- –≥–ª–∞–≤–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä: PROPN / NER / —á–∞—Å—Ç–æ—Ç–∞ ---
        doc_line = nlp(cleaned)
        tokens_alpha = [t for t in doc_line if t.is_alpha]
        tokens_lower = [t.text.lower() for t in tokens_alpha]

        # 0) –Ø–≤–Ω—ã–π –º—É—Å–æ—Ä —Ç–∏–ø–∞ "–ö–û–ù–ï–¶ –ü–ï–†–í–û–ô –°–ï–†–ò–ò" ‚Äî –∫–∞–∫ —É —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å
        if any(w in tokens_lower for w in ("–∫–æ–Ω–µ—Ü", "—Å–µ—Ä–∏–∏", "—Å–µ—Ä–∏—è", "—Å–µ–∑–æ–Ω–∞", "—Å–µ–∑–æ–Ω")):
            offset += len(raw)
            continue

        # üîπ 1) –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞: –ì–†–û–ú–ö–û, –®–£–¢–õ–ò–í–û, –í–í–ï–†–•–£, –í–°–ï–ú
        if len(tokens_alpha) == 1 and tokens_lower[0] in STOP_SINGLE:
            offset += len(raw)
            continue

        # üîπ 2) –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–≥–æ–≤—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏: "–ù–ê –°–¢–ï–ù–ê", "–ù–ê –°–¢–ï–ù–ï", "–í –í–ï–†–•–£"
        if len(tokens_alpha) <= 2 and tokens_lower and tokens_lower[0] in PREPOSITIONS:
            offset += len(raw)
            continue


        has_propn = any(t.pos_ == "PROPN" for t in tokens_alpha)
        has_ner_match = any(tok in per_tokens for tok in tokens_lower)
        has_freq = any(token_freq.get(tok, 0) > 1 for tok in tokens_lower)

        # üîπ –ù–û–í–û–ï: —Ä–æ–ª–µ–≤—ã–µ "–≥–æ–ª–æ–≤—ã" ‚Äî –ß–ò–ù–û–í–ù–ò–ö, –°–û–¢–†–£–î–ù–ò–ö –∏ —Ç.–ø.
        # –µ—Å–ª–∏ —ç—Ç–æ –æ–¥–Ω–∞ —Ç–∞–∫–∞—è —Ä–æ–ª—å + –Ω–æ–º–µ—Ä ‚Üí —Ç–æ–∂–µ —Å—á–∏—Ç–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–º —Å–ø–∏–∫–µ—Ä–æ–º
        head = tokens_alpha[0].text.upper() if tokens_alpha else ""
        is_role_speaker = (
            head in ROLE_SPEAKER_HEADS
            and len(tokens_alpha) <= 2   # –ß–ò–ù–û–í–ù–ò–ö –∏–ª–∏ –ß–ò–ù–û–í–ù–ò–ö 1
        )

        if not (has_propn or has_ner_match or has_freq or is_role_speaker):
            offset += len(raw)
            continue

        # –µ—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ ‚Äî —ç—Ç–æ –ø–µ—Ä—Å–æ–Ω–∞–∂
        start_char = offset
        end_char = offset + len(line)
        speakers.append((cleaned, i, start_char, end_char))

        offset += len(raw)

    return speakers

# -------------------- –®–∞–≥ 2. –ò–º–µ–Ω–∞ –∏–∑ "—à–∞–ø–∫–∏" –∫–∞—Å—Ç–∏–Ω–≥–∞ --------------------

def extract_prim_names(text: str) -> List[Tuple[str, int, int, int]]:
    """
    –ò—â–µ–º –±–ª–æ–∫–∏ –≤–∏–¥–∞:
      (–ü–†–ò–ú: ... –ö–∞—Ç—è, –õ–µ–≤, –ú–∞–∫—Å, –ú–∞—Ç–≤–µ–π, ...)

    –í–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –≤–∏–¥–∞ [–ê-–Ø–Å][–∞-—è—ë]+
    –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞.

    –í–∞–∂–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ:
      - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¢–û–õ–¨–ö–û —Ç–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ö–æ–∂–∏
        –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã –ª—é–¥–µ–π (–µ—Å—Ç—å '–∫–æ–º–ø–∞–Ω–∏—è', '—á–µ–ª–æ–≤–µ–∫', —Ü–∏—Ñ—Ä—ã –∏ —Ç.–ø.).
    """

    prim_pat = re.compile(r"\(\s*–ü–†–ò–ú[^)]*\)", flags=re.IGNORECASE | re.DOTALL)
    name_pat = re.compile(r"\b[–ê-–Ø–Å][–∞-—è—ë]+\b")

    # —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –Ω–µ –∏–º–µ–Ω–∞ –≤ —Ç–∞–∫–∏—Ö –ø—Ä–∏–º–µ—á–∞–Ω–∏—è—Ö
    STOP = {
        "–ø—Ä–∏–º", "–æ—Å–Ω–æ–≤–Ω–∞—è", "–æ—Å–Ω–æ–≤–Ω–æ–π", "–∫–æ–º–ø–∞–Ω–∏—è",
        "—á–µ–ª–æ–≤–µ–∫", "—á–µ–ª–æ–≤–µ–∫–æ–≤", "—á–µ–ª–æ–≤–µ–∫–∞", "–ª—é–¥–µ–π", "—á–µ–ª",
        "–ø–ª—é—Å", "–µ—â–µ", "–µ—â—ë",
        "–∫–æ—Ç–æ—Ä–æ–≥–æ", "–∫–æ—Ç–æ—Ä—ã–µ", "–∫–æ—Ç–æ—Ä—ã–π",
        "–≤–µ—Ä–Ω—É—Ç", "—Å—Ä–∞–∑—É", "–æ—Ç—Å—Ç–∞–Ω—É—Ç", "–ø–æ–±–µ–≥–∞", "–ø–æ—Ä–æ–≥",
        "–æ—Å–Ω–æ–≤–Ω–∞—è", "–∫–æ–º–ø–∞–Ω–∏—è",
    }

    results: List[Tuple[str, int, int, int]] = []

    for m in prim_pat.finditer(text):
        block = m.group(0)          # '(–ü–†–ò–ú: ... )'
        inner = block[1:-1]         # –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö —Å–∫–æ–±–æ–∫
        inner_lower = inner.lower()

        # üîπ –ù–æ–≤—ã–π —Ñ–∏–ª—å—Ç—Ä: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ü–†–ò–ú, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ö–æ–∂–∏
        #    –Ω–∞ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ / –≥—Ä—É–ø–ø—ã –ª—é–¥–µ–π
        if not re.search(r"\d", inner_lower) and not any(
            key in inner_lower
            for key in ("–∫–æ–º–ø–∞–Ω", "—á–µ–ª–æ–≤–µ–∫", "—á–µ–ª", "—Ä–µ–±—è—Ç", "–ø–æ–¥—Ä–æ—Å—Ç", "–ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤")
        ):
            # –ø—Ä–∏–º–µ—Ä: (–ü–†–ò–ú: –¶–∏—Ç–∞—Ç–∞ –∏–∑ ¬´–ö–∞–∫ –æ–Ω–∞ –º–µ–Ω—è –≤—ã–Ω–æ—Å–∏—Ç¬ª –ú–∞—Ç–∞–Ω–≥–∞)
            # —Ç—É—Ç –Ω–µ—Ç –Ω–∏ —Ü–∏—Ñ—Ä, –Ω–∏ '–∫–æ–º–ø–∞–Ω–∏—è', –Ω–∏ '—á–µ–ª–æ–≤–µ–∫' ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
            continue

        for nm in name_pat.finditer(inner):
            word = nm.group(0)
            if word.lower() in STOP:
                continue

            # –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            global_start = m.start() + nm.start()
            global_end = global_start + len(word)
            line_idx = text.count("\n", 0, global_start)

            results.append((word, line_idx, global_start, global_end))

    return results


def extract_header_names(
    text: str,
) -> List[Tuple[str, int, int, int]]:
    """
    –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏-—à–∞–ø–∫–∏ —Å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ–º –∏–º—ë–Ω:
      –°–û–ö–û–õ–û–í, –ö–û–ú–°–û–ú–û–õ–ö–ê, –°–û–¢–†–£–î–ù–ò–ö-1 –ì–†–ê–ñ–î–ê–ù–°–ö–û–ô –ê–í–ò–ê–¶–ò–ò, ...

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (name, line_idx, start_char, end_char),
    –≥–¥–µ name ‚Äî –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç —à–∞–ø–∫–∏.

    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:
      - –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –º–∞—Å—Å–æ–≤–∫—É '–ß–£–ö–ß–ò (10 –ß–ï–õ)' –∏ '–ö–û–ú–°–û–ú–û–õ–ö–ê (18‚Äì25)',
      - –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —à—É–º: '–ì–†–û–ú–ö–û', '–®–£–¢–õ–ò–í–û', '–í–°–ï–ú', '–í–í–ï–†–•–£',
      - –≤—ã–∫–∏–¥—ã–≤–∞–µ–º '–ö–û–ù–ï–¶ –ü–ï–†–í–û–ô –°–ï–†–ò–ò' –∏ –ø–æ–¥–æ–±–Ω—ã–µ,
      - –æ–±—Ä–µ–∑–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ '–ó/–ö', '–ó–ö' –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–∫–æ–±–æ–∫: '–†–ò–¢–ê –ó/–ö' ‚Üí '–†–ò–¢–ê'.
    """
    results: List[Tuple[str, int, int, int]] = []

    time_pat = re.compile(r"\(\s*\d{1,2}:\d{2}(?::\d{2})?\s*\)")
    lines = text.splitlines(keepends=True)
    offset = 0

    for i, raw in enumerate(lines):
        line = raw.rstrip("\n")
        s = line.strip()
        if not s:
            offset += len(raw)
            continue

        if re.match(r"^[A-Z–ê-–Ø–Å0-9]+(?:\s+[A-Z–ê-–Ø–Å0-9]+)*\s*\(", s):
            offset += len(raw)
            continue

        # —É–±–∏—Ä–∞–µ–º —Ç–∞–π–º–∫–æ–¥ –≤–∏–¥–∞ (01:10)
        s_head = time_pat.sub("", s)

        # –Ω—É–∂–Ω–∞ –∑–∞–ø—è—Ç–∞—è: —ç—Ç–æ —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Å–ø–∏—Å–æ–∫
        if "," not in s_head:
            offset += len(raw)
            continue

        # –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —Å—Ç—Ä–æ–∫–∞ "–∑–∞–≥–ª–∞–≤–Ω–∞—è", –∫–∞–∫ —É —à–∞–ø–∫–∏
        letters = re.sub(rf"[^A-Za-z{RUS_UP}{RUS_LO}–Å—ë]", "", s_head)
        if not letters:
            offset += len(raw)
            continue
        upper = sum(1 for ch in letters if ch == ch.upper())
        upper_ratio = upper / len(letters)
        if upper_ratio < 0.6:
            offset += len(raw)
            continue

        # –æ—Ç—Ä–µ–∂–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ü–µ–Ω—ã (–¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–æ—á–∫–∏)
        chunk = s_head.split(".")[-1].strip() if "." in s_head else s_head

        # –µ—Å–ª–∏ –µ—Å—Ç—å –ú–ê–°–°–û–í–ö–ê:, –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ª–µ–≤—É—é —á–∞—Å—Ç—å –¥–æ –Ω–µ—ë
        mass_split = re.split(r"\b–ú–ê–°–°–û–í–ö–ê\b\s*:", chunk, maxsplit=1, flags=re.IGNORECASE)
        main_part = mass_split[0].strip()

        # –µ—Å–ª–∏ –≤—Å—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–æ –∫–æ–Ω–µ—Ü —Å–µ—Ä–∏–∏/—Å–µ–∑–æ–Ω–∞ ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
        if re.search(r"\b–ö–û–ù–ï–¶\b.*\b–°–ï–†–ò", main_part, flags=re.IGNORECASE):
            offset += len(raw)
            continue

        # –≤ main_part –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if any(p in main_part for p in [".", "!", "?", ":"]):
            offset += len(raw)
            continue

        for part in main_part.split(","):
            name = part.strip().strip(".:()")
            if not name:
                continue


            # 1) –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —á–∏—Å—Ç—É—é –º–∞—Å—Å–æ–≤–∫—É '–ß–£–ö–ß–ò (10 –ß–ï–õ)' –ø—Ä—è–º–æ –≤ —à–∞–ø–∫–µ
            if re.search(
                r"\(\s*\d+\s*(?:—á–µ–ª|—á–µ–ª–æ–≤–µ–∫|—á–µ–ª–æ–≤–µ–∫–∞)\s*\)",
                name,
                flags=re.IGNORECASE,
            ):
                continue

            # 2) –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —è–≤–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç: '–ö–û–ú–°–û–ú–û–õ–ö–ê (18-25)'
            if re.search(r"\(\s*\d+\s*[-‚Äì]\s*\d+\s*\)", name):
                continue

            # 3) —á–∏—Å—Ç–∏–º —Å–∫–æ–±–∫–∏: '–†–ò–¢–ê (–í–°–ï–ú' ‚Üí '–†–ò–¢–ê'
            name = re.sub(r"\(.*", "", name).strip()

            # 4) –æ–±—Ä–µ–∑–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ '–ó/–ö', '–ó–ö' –∏ —Ö–≤–æ—Å—Ç –ø–æ—Å–ª–µ –Ω–µ–≥–æ:
            #    '–†–ò–¢–ê –ó/–ö' ‚Üí '–†–ò–¢–ê', '–ü–ê–†–ï–ù–¨ 1 –ó/–ö' ‚Üí '–ü–ê–†–ï–ù–¨ 1'
            name = re.sub(r"\s+–ó\s*/?\s*[–öK]\b.*", "", name, flags=re.IGNORECASE).strip()

            if not name:
                continue

            low_name = name.lower()

            # 5) —è–≤–Ω—ã–π –º—É—Å–æ—Ä –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –Ω–∞–¥–ø–∏—Å–µ–π, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –¥–æ–ª–µ—Ç–µ–ª
            if ("–∫–æ–Ω–µ—Ü" in low_name and "—Å–µ—Ä–∏" in low_name) or "—Å–µ–∑–æ–Ω" in low_name:
                continue

            tokens = [t for t in name.split() if t]
            if not tokens:
                continue

            if len(tokens) >= 2:
                first = tokens[0].lower()
                if re.search(r"(–µ—Ç|—ë—Ç|–∏—Ç|–∞–ª|–∞–ª–∞|–∞–ª–∏|—é—Ç|—É—Ç|–µ—à—å|–∞–µ—à—å|–∞–µ—Ç|—è–µ—Ç)$", first):
                    # –æ—á–µ–Ω—å –≥—Ä—É–±–æ, –Ω–æ '–±—å–µ—Ç', '–≥–æ–≤–æ—Ä–∏—Ç', '–∏–¥–µ—Ç', '—Å—Ç—Ä–µ–ª—è–µ—Ç' —Å—é–¥–∞ –ø–æ–ø–∞–¥—É—Ç
                    continue

            # –ª—ë–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–æ—Ä–º—É –∏–º–µ–Ω–∏: —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–ª–∏ –ö–ê–ü–°, –∏–ª–∏ TitleCase
            ok_tokens = 0
            for t in tokens:
                if t.isupper() or t.istitle():
                    ok_tokens += 1
            if ok_tokens / len(tokens) < 0.7:
                # —ç—Ç–æ —á—Ç–æ-—Ç–æ —Ç–∏–ø–∞ "—Ç—ã –Ω–µ" –∏ –ø—Ä–æ—á–∏–π –º—É—Å–æ—Ä
                continue

            letters_name = re.sub(rf"[^A-Za-z{RUS_UP}{RUS_LO}–Å—ë]", "", name)
            if len(letters_name) < 2:
                continue

            # –µ—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ ‚Äî —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç —à–∞–ø–∫–∏
            start_in_line = line.index(name)
            start_char = offset + start_in_line
            end_char = start_char + len(name)
            results.append((name, i, start_char, end_char))

        offset += len(raw)

    return results



def _normalize_mass_label(label: str) -> str:
    """
    '–ú–∞—Å—Å–æ–≤–∫–∞ - —á–µ–ª—é—Å–∫–∏–Ω—Ü—ã' -> '–ß–µ–ª—é—Å–∫–∏–Ω—Ü—ã'
    '–º–∞—Å—Å–æ–≤–∫–∞: —Ç—É—Ä–∏—Å—Ç—ã'     -> '–¢—É—Ä–∏—Å—Ç—ã'
    """
    s = _norm(label)
    # —É–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "–º–∞—Å—Å–æ–≤–∫–∞", "–º–∞—Å—Å–æ–≤–∫–∞ -" –∏ "–º–∞—Å—Å–æ–≤–∫–∞:"
    s = re.sub(r"(?i)^–º–∞—Å—Å–æ–≤–∫–∞\s*[-:‚Äì‚Äî]\s*", "", s)
    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–≥–∏—Å—Ç—Ä: –ø–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –∫–∞–∫ –≤ lower()
    if not s:
        return s
    s = s.lower()
    return s[:1].upper() + s[1:]


def _extract_massovka(text: str) -> Set[str]:
    ms = set()
    # A) ‚Äú–ú–ê–°–°–û–í–ö–ê: ‚Ä¶ (N)‚Äù ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
    for grp in re.findall(r"–ú–ê–°–°–û–í–ö–ê[:\-\s]+(.+)", text, flags=re.IGNORECASE):
        for label, num in re.findall(
            rf"([{RUS_UP}{RUS_LO} \-]+?)\s*\(\s*(\d+)",
            grp
        ):
            ms.add(f"{_norm(label).capitalize()} ({int(num)})")

    # B) –í –ª—é–±–æ–º –º–µ—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞ ‚Äî –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫ –µ—Å—Ç—å –º–∞—Ä–∫–µ—Ä –ª—é–¥–µ–π
    for label, num in re.findall(
        rf"([{RUS_UP}{RUS_LO} \-]+?)\s*\(\s*(\d+)\s*(?:—á–µ–ª|—á–µ–ª–æ–≤–µ–∫|—á–µ–ª–æ–≤–µ–∫–∞)\s*\)",
        text,
        flags=re.IGNORECASE,
    ):
        ms.add(f"{_norm(label).capitalize()} ({int(num)})")

    return ms

from typing import Tuple, Set


def _extract_massovka_and_grouping(text: str) -> Tuple[Set[str], Set[str]]:
    massovka = set()
    grouping = set()

    # A) "–ú–ê–°–°–û–í–ö–ê: –ß–£–ö–ß–ò (10 –ß–ï–õ), –¢–ï–•–ù–ò–ö–ò (2 –ß–ï–õ)"
    for grp in re.findall(r"–ú–ê–°–°–û–í–ö–ê[:\-\s]+(.+)", text, flags=re.IGNORECASE):
        matches = re.findall(
            rf"([{RUS_UP}{RUS_LO} \-]+?)\s*\(\s*(\d+)\s*(?:—á–µ–ª|—á–µ–ª–æ–≤–µ–∫|—á–µ–ª–æ–≤–µ–∫–∞)\s*\)",
            grp,
            flags=re.IGNORECASE,
        )
        for idx, (label, num) in enumerate(matches):
            norm_label = _normalize_mass_label(label)
            if not norm_label:
                continue
            item = f"{norm_label} ({int(num)})"
            if idx == 0:
                massovka.add(item)   # –ø–µ—Ä–≤—ã–π ‚Äî –º–∞—Å—Å–æ–≤–∫–∞
            else:
                grouping.add(item)   # –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –≥—Ä—É–ø–ø–æ–≤–∫–∞

    # B) –û—Å—Ç–∞–ª—å–Ω—ã–µ "–•–•–• (N —á–µ–ª)" –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É
    for label, num in re.findall(
        rf"([{RUS_UP}{RUS_LO} \-]+?)\s*\(\s*(\d+)\s*(?:—á–µ–ª|—á–µ–ª–æ–≤–µ–∫|—á–µ–ª–æ–≤–µ–∫–∞)\s*\)",
        text,
        flags=re.IGNORECASE,
    ):
        norm_label = _normalize_mass_label(label)
        if not norm_label:
            continue
        item = f"{norm_label} ({int(num)})"
        if item in massovka or item in grouping:
            continue
        massovka.add(item)

    return massovka, grouping


def extract_implicit_massovka(
    text: str,
    object_: str,
    subobject: str,
    nlp,
    *,
    n_chars: int | None = None,
    min_chars: int = 3,
) -> set[str]:
    """
    –°–∫—Ä—ã—Ç–∞—è –º–∞—Å—Å–æ–≤–∫–∞:
      - –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Ñ–æ–Ω–æ–≤—ã–µ –≥—Ä—É–ø–ø—ã –ª—é–¥–µ–π ('—Ç–æ–ª–ø–∞', '–ª—é–¥–∏', '—Ç—É—Ä–∏—Å—Ç—ã' –∏ —Ç.–ø.),
      - –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏:
          * —Å—Ü–µ–Ω–∞ –≤ –ø—É–±–ª–∏—á–Ω–æ–º –º–µ—Å—Ç–µ (–≥–æ—Ä–æ–¥ / —É–ª–∏—Ü–∞ / –ø–ª–æ—â–∞–¥—å / –ø–∞—Ä–∫ / –º–µ—Ç—Ä–æ ...),
          * –∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ —Å—Ü–µ–Ω–µ –º–∞–ª–æ (n_chars <= min_chars),
            –µ—Å–ª–∏ n_chars –ø–µ—Ä–µ–¥–∞–Ω.
    """

    # 1) –µ—Å–ª–∏ —è–≤–Ω–æ –º–Ω–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π ‚Äî –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º —Å–∫—Ä—ã—Ç—É—é –º–∞—Å—Å–æ–≤–∫—É
    if n_chars is not None and n_chars > min_chars:
        return set()

    # 2) –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–æ–∫–∞—Ü–∏—è "–ø—É–±–ª–∏—á–Ω–∞—è"
    loc_text = f"{object_ or ''} {subobject or ''}".strip()
    if loc_text:
        doc_loc = nlp(loc_text)
        loc_lemmas = {t.lemma_.lower() for t in doc_loc if t.is_alpha}
        if not (loc_lemmas & PUBLIC_LOCATION_LEMMAS):
            # –Ω–µ –≥–æ—Ä–æ–¥/—É–ª–∏—Ü–∞/–ø–ª–æ—â–∞–¥—å/–º–µ—Ç—Ä–æ –∏ —Ç.–ø. ‚Üí –Ω–µ —Å—á–∏—Ç–∞–µ–º –º–∞—Å—Å–æ–≤–∫—É
            return set()
    else:
        # –≤–æ–æ–±—â–µ –Ω–µ—Ç –æ–±—ä–µ–∫—Ç–∞/–ø–æ–¥–æ–±—ä–µ–∫—Ç–∞ ‚Üí –ª—É—á—à–µ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å –º–∞—Å—Å–æ–≤–∫—É
        return set()

    # 3) –∏—â–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –≥—Ä—É–ø–ø–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ
    doc = nlp(text)
    found: set[str] = set()

    for tok in doc:
        if not tok.is_alpha:
            continue
        lemma = tok.lemma_.lower()
        if lemma in IMPLICIT_MASS_GROUP_LEMMAS:
            found.add(lemma.capitalize())  # '—Ç–æ–ª–ø–∞' ‚Üí '–¢–æ–ª–ø–∞'

    return found

SMALL_GROUP_NUM_WORDS = r"(–¥–≤–æ–µ|—Ç—Ä–æ–µ|—á–µ—Ç–≤–µ—Ä–æ|–ø—è—Ç–µ—Ä–æ|—à–µ—Å—Ç–µ—Ä–æ|—Å–µ–º–µ—Ä–æ|–≤–æ—Å—å–º–µ—Ä–æ|–¥–µ–≤—è—Ç–µ—Ä–æ|–¥–µ—Å—è—Ç–µ—Ä–æ)"

# –≥—Ä—É–ø–ø–æ–≤—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏–º —Å—á–∏—Ç–∞—Ç—å "–≥—Ä—É–ø–ø–æ–≤–∫–æ–π"
TEXT_GROUP_NOUNS = {
    "–æ—Ç—Ä—è–¥",
    "—É—á–∞—Å—Ç–Ω–∏–∫–∏",
    "—É—á–∞—Å—Ç–Ω–∏–∫",
    # —Å—é–¥–∞ –∂–µ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å "–≥—Ä—É–ø–ø–∞", "–∫–æ–º–∞–Ω–¥–∞" –∏ —Ç.–ø., –µ—Å–ª–∏ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è
}

def _extract_small_numeric_grouping(text: str, nlp=None) -> Set[str]:
    res: Set[str] = set()

    # 1) –ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ + —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ: "–¥–≤–æ–µ –º—É–∂—á–∏–Ω", "—Ç—Ä–æ–µ –ø–∞—Ä–Ω–µ–π"...
    pattern = re.compile(
        rf"\b{SMALL_GROUP_NUM_WORDS}\s+([A-Za-z–ê-–Ø–∞-—è–Å—ë]+)",
        flags=re.IGNORECASE
    )

    for m in pattern.finditer(text):
        num_word = m.group(1)
        noun    = m.group(2)

        phrase = f"{num_word} {noun}".strip()
        phrase_norm = phrase[:1].upper() + phrase[1:].lower()
        res.add(phrase_norm)

    # 2) –û–¥–∏–Ω–æ—á–Ω—ã–µ –≥—Ä—É–ø–ø–æ–≤—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ: "–æ—Ç—Ä—è–¥", "—É—á–∞—Å—Ç–Ω–∏–∫–∏"/"—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤" –∏ —Ç.–ø.
    lemma_groups: Set[str] = set()

    if nlp is not None:
        doc = nlp(text)
        for tok in doc:
            if not tok.is_alpha:
                continue
            lemma = tok.lemma_.lower()
            if lemma in TEXT_GROUP_NOUNS:
                lemma_groups.add(lemma)
    else:
        # fallback –±–µ–∑ spaCy: –≥—Ä—É–±–æ –∏—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
        lower_text = text.lower()
        for lemma in TEXT_GROUP_NOUNS:
            # –∏—â–µ–º –ª—é–±–æ–π "—Ö–≤–æ—Å—Ç" —Ñ–æ—Ä–º—ã: —É—á–∞—Å—Ç–Ω–∏–∫/—É—á–∞—Å—Ç–Ω–∏–∫–∏/—É—á–∞—Å—Ç–Ω–∏–∫–æ–≤..., –æ—Ç—Ä—è–¥/–æ—Ç—Ä—è–¥–∞...
            if re.search(rf"\b{lemma}\w*\b", lower_text):
                lemma_groups.add(lemma)

    # 3) –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–æ–Ω—ã –ø–æ –ª–µ–º–º–∞–º (–æ–¥–Ω–∞ —Ñ–æ—Ä–º–∞ –Ω–∞ –ª–µ–º–º—É)
    for lemma in lemma_groups:
        canon = TEXT_GROUP_CANON.get(lemma)
        if canon:
            res.add(canon)

    return res

# -------------------- –®–∞–≥ 3. NER (—Ç–æ–ª—å–∫–æ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ –≥–µ—Ä–æ—è–º) --------------------

def extract_ner_persons(
    text: str,
    nlp,
    anchor_names: List[str],
) -> List[Tuple[str, int, int, int]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ–º NER-PER.

    –û—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∂–∏–º (–µ—Å–ª–∏ –µ—Å—Ç—å anchor_names):
      - —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –ª–µ–º–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω—ã c anchor_names,
      - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–π—Å—è —Ñ–∞–º–∏–ª–∏–µ–π,
      - —Å—Ä–µ–∑–∞–µ–º –≤–µ–¥—É—â–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏: '–ö –ê–ª–∏—Å–µ' -> '–ê–ª–∏—Å–µ'.

    Fallback-—Ä–µ–∂–∏–º (–µ—Å–ª–∏ —è–∫–æ—Ä–µ–π –Ω–µ—Ç):
      - –±–µ—Ä—ë–º –≤—Å–µ PER —Å PROPN –≤–Ω—É—Ç—Ä–∏,
      - —Å—Ä–µ–∑–∞–µ–º –≤–µ–¥—É—â–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —á–∏—Å—Ç–∏–º –∏–º—è.
    """
    doc = nlp(text)

    # --- –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ line_offsets –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–æ–∫–∏ ---
    lines = text.splitlines(keepends=True)
    line_offsets = []
    offset = 0
    for raw in lines:
        line_offsets.append(offset)
        offset += len(raw)

    def line_index_from_pos(pos: int) -> int:
        idx = 0
        for i, off in enumerate(line_offsets):
            if off <= pos:
                idx = i
            else:
                break
        return idx

    results: List[Tuple[str, int, int, int]] = []

    # --- 1) –ª–µ–º–º—ã —è–∫–æ—Ä–Ω—ã—Ö –∏–º—ë–Ω ---
    anchor_lemmas: Set[str] = set()
    anchor_last_tokens: Set[str] = set()
    for name in anchor_names:
        d = nlp(name)
        tokens = [t for t in d if t.is_alpha]
        for t in tokens:
            anchor_lemmas.add(t.lemma_.lower())
        if tokens:
            anchor_last_tokens.add(tokens[-1].text.lower())

    # === Fallback-—Ä–µ–∂–∏–º: —è–∫–æ—Ä–µ–π –Ω–µ—Ç ‚Üí –±–µ—Ä—ë–º –≤—Å–µ PER ===
    if not anchor_lemmas:
        for ent in doc.ents:
            if ent.label_ != "PER":
                continue

            ent_tokens = [t for t in ent if t.is_alpha]
            if not ent_tokens:
                continue

            # üîπ –∑–∞—â–∏—Ç–∞ –æ—Ç –≥–ª–∞–≥–æ–ª—å–Ω—ã—Ö "–∏–º—ë–Ω" —Ç–∏–ø–∞ "–ë–µ–∂–∏—Ç"
            # –µ—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏ spaCy —Å—á–∏—Ç–∞–µ—Ç –µ–≥–æ VERB/AUX ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º
            if len(ent_tokens) == 1 and ent_tokens[0].pos_ in ("VERB", "AUX"):
                continue

            has_propn = any(t.pos_ == "PROPN" for t in ent_tokens)

            # –µ—Å–ª–∏ –Ω–µ—Ç PROPN, –≤—Å—ë —Ä–∞–≤–Ω–æ —Ä–∞–∑—Ä–µ—à–∞–µ–º –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏,
            # –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –≥–ª–∞–≥–æ–ª–æ–º:
            # –ú–∞–∫—Å, –¢–∏–º—É—Ä, –ú–∞—Ç–≤–µ–π –∏ —Ç.–ø.
            if not has_propn:
                if not (len(ent_tokens) == 1 and ent_tokens[0].text[:1].isupper()):
                    continue

            # –¥–∞–ª—å—à–µ –∫–∞–∫ –±—ã–ª–æ: –æ–±—Ä–µ–∑–∫–∞ –ø—Ä–µ–¥–ª–æ–≥–æ–≤, _clean_person_name, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ results
            start_idx = 0
            while (
                start_idx < len(ent_tokens)
                and ent_tokens[start_idx].text.lower() in PREPOSITIONS
            ):
                start_idx += 1

            core_tokens = ent_tokens[start_idx:] if start_idx < len(ent_tokens) else ent_tokens
            if not core_tokens:
                continue

            core_text = " ".join(t.text for t in core_tokens)
            name_raw = _clean_person_name(core_text)
            if not name_raw:
                continue

            line_idx = line_index_from_pos(ent.start_char)
            results.append((name_raw, line_idx, ent.start_char, ent.end_char))

        return results



    # === –û—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∂–∏–º: –µ—Å—Ç—å anchor_lemmas ‚Üí –∂—ë—Å—Ç–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –ø–æ —è–∫–æ—Ä—è–º ===
    for ent in doc.ents:
        if ent.label_ != "PER":
            continue
        if not any(t.pos_ == "PROPN" for t in ent):
            continue

        # —Å–≤—è–∑—å –ø–æ –ª–µ–º–º–∞–º —Å —è–∫–æ—Ä—è–º–∏
        ent_lemmas = {t.lemma_.lower() for t in ent if t.is_alpha}
        if not ent_lemmas & anchor_lemmas:
            continue

        # —Ç–æ–∫–µ–Ω—ã —ç–Ω—Ç–∏—Ç–∏, –ø—Ä–∏–≥–æ–¥–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        ent_tokens = [t for t in ent if t.is_alpha]

        # —Ñ–∏–ª—å—Ç—Ä "–ø–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è —Ñ–∞–º–∏–ª–∏—è" –¥–ª—è –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π
        if len(ent_tokens) > 1:
            last_tok = ent_tokens[-1].text.lower()
            if last_tok in anchor_last_tokens:
                continue

        if len(ent_tokens) == 1:
            if ent_tokens[0].lemma_.lower() in anchor_lemmas:
                continue

        # --- –æ–±—Ä–µ–∑–∞–µ–º –≤–µ–¥—É—â–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏: '–ö –ê–ª–∏—Å–µ' -> '–ê–ª–∏—Å–µ' ---
        start_idx = 0
        while (
            start_idx < len(ent_tokens)
            and ent_tokens[start_idx].text.lower() in PREPOSITIONS
        ):
            start_idx += 1

        core_tokens = ent_tokens[start_idx:] if start_idx < len(ent_tokens) else ent_tokens
        if not core_tokens:
            continue

        # —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ "—è–¥—Ä–∞" (–±–µ–∑ –ø—Ä–µ–¥–ª–æ–≥–æ–≤)
        core_text = " ".join(t.text for t in core_tokens)
        name_raw = _clean_person_name(core_text)
        if not name_raw:
            continue

        line_idx = line_index_from_pos(ent.start_char)
        results.append((name_raw, line_idx, ent.start_char, ent.end_char))

    return results


# -------------------- –®–∞–≥ 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏–º—ë–Ω --------------------

DIMINUTIVE_SUFFIXES = (
    "—à–∞", "–∫–∞", "–æ—á–∫–∞", "–µ—á–∫–∞", "–µ–Ω—å–∫–∞", "—é—à–∫–∞", "—é—à–∞",
    "–∏–∫", "—á–∏–∫", "—ë–∫", "–µ–∫", "—é—Ö–∞", "—é–Ω—è", "—É–ª—è", "–∏–Ω–∫–∞", "–æ–Ω—å–∫–∞"
)

PATRONYMIC_SUFFIXES = {"—ã—á", "—ã—á–∞", "–∏—á", "–∏—á–∞"}


def _nickname_matches_base(a: str, b: str) -> bool:
    """
    –ü—Ä–æ–∑–≤–∏—â–∞ / —É–º–µ–Ω—å—à–∏—Ç–µ–ª—å–Ω—ã–µ:
      - –ò–≤–∞–Ω ~ –ò–≤–∞–Ω—ã—á
      - –ê—Ä–∏–Ω–∞ ~ –ê—Ä–∏—à–∞
      - –ú–∏—Ç—è ~ –ú–∏—Ç—å–∫–∞ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ)
    –†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã—Ö –∏–º—ë–Ω.
    """

    a_l, b_l = a.lower(), b.lower()
    if a_l == b_l:
        return False
    if len(a_l) < 3 or len(b_l) < 3:
        return False

    # 1) –ø–∞—Ç—Ä–æ–Ω–∏–º–∏—á–µ—Å–∫–∏–µ: –ò–≤–∞–Ω ~ –ò–≤–∞–Ω—ã—á
    #   (–¥–ª–∏–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞ = –∫–æ—Ä–æ—Ç–∫–∞—è + —Å—É—Ñ—Ñ–∏–∫—Å –∏–∑ PATRONYMIC_SUFFIXES)
    for short, long in ((a_l, b_l), (b_l, a_l)):
        if len(long) <= len(short):
            continue
        if long.startswith(short):
            suf = long[len(short):]
            if suf in PATRONYMIC_SUFFIXES:
                return True

    # 2) —É–º–µ–Ω—å—à–∏—Ç–µ–ª—å–Ω—ã–µ: –æ–±—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å ‚â• 3 –±—É–∫–≤, –∞ –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ö–≤–æ—Å—Ç —É –æ–¥–Ω–æ–≥–æ ‚Äî —Ç–∏–ø–∏—á–Ω—ã–π
    #    diminutive —Å—É—Ñ—Ñ–∏–∫—Å, –∞ —É –¥—Ä—É–≥–æ–≥–æ ‚Äî "–∂–µ–Ω—Å–∫–∏–π" —Ö–≤–æ—Å—Ç (–∞/—è/–Ω–∞) –∏–ª–∏ –ø—É—Å—Ç–æ.
    def common_prefix_len(x: str, y: str) -> int:
        n = min(len(x), len(y))
        i = 0
        while i < n and x[i] == y[i]:
            i += 1
        return i

    cp = common_prefix_len(a_l, b_l)
    if cp < 3:
        return False

    tail_a = a_l[cp:]
    tail_b = b_l[cp:]

    # –¥–æ–ø—É—Å—Ç–∏–º—ã–µ "–±–∞–∑–æ–≤—ã–µ" –æ–∫–æ–Ω—á–∞–Ω–∏—è —É –ø–æ–ª–Ω–æ–≥–æ –∏–º–µ–Ω–∏
    BASE_ENDINGS = {"", "–∞", "—è", "–Ω–∞"}

    # –≤–∞—Ä–∏–∞–Ω—Ç 1: a = –±–∞–∑–∞, b = —É–º–µ–Ω—å—à–∏—Ç–µ–ª—å–Ω–æ–µ
    if tail_a in BASE_ENDINGS and tail_b in DIMINUTIVE_SUFFIXES:
        return True
    # –≤–∞—Ä–∏–∞–Ω—Ç 2: b = –±–∞–∑–∞, a = —É–º–µ–Ω—å—à–∏—Ç–µ–ª—å–Ω–æ–µ
    if tail_b in BASE_ENDINGS and tail_a in DIMINUTIVE_SUFFIXES:
        return True

    return False

def _same_name_case_variant(a: str, b: str) -> bool:
    """
    –ü–∞–¥–µ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –∏–º–µ–Ω–∏:
    –ê—Ä–∏–Ω–∞ ~ –ê—Ä–∏–Ω—É ~ –ê—Ä–∏–Ω–µ, –ü—Ä–æ—Ö–æ—Ä ~ –ü—Ä–æ—Ö–æ—Ä–∞, –°–æ–∫–æ–ª–æ–≤ ~ –°–æ–∫–æ–ª–æ–≤–∞ –∏ —Ç.–ø.

    –†–∞–±–æ—Ç–∞–µ—Ç –¢–û–õ–¨–ö–û –¥–ª—è –æ–¥–Ω–æ—Å–ª–æ–≤–Ω—ã—Ö –∏–º—ë–Ω.
    –ò–¥–µ—è:
      1) –¥–ª–∏–Ω–Ω—ã–π –æ–±—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å,
      2) —Ä–∞–∑–ª–∏—á–∏—è —Ç–æ–ª—å–∫–æ –≤ —Ö–≤–æ—Å—Ç–µ –∏–∑ —Ç–∏–ø–∏—á–Ω—ã—Ö –ø–∞–¥–µ–∂–Ω—ã—Ö –≥–ª–∞—Å–Ω—ã—Ö/—Å—É—Ñ—Ñ–∏–∫—Å–æ–≤.
    """
    a_l, b_l = a.lower(), b.lower()
    if len(a_l) < 3 or len(b_l) < 3:
        return False

    # 1) —Å—á–∏—Ç–∞–µ–º –¥–ª–∏–Ω—É –æ–±—â–µ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞
    n = min(len(a_l), len(b_l))
    i = 0
    while i < n and a_l[i] == b_l[i]:
        i += 1

    # –æ–±—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å –¥–æ–ª–∂–µ–Ω –ø–æ–∫—Ä—ã–≤–∞—Ç—å —Ö–æ—Ç—è –±—ã (min_len - 1) —Å–∏–º–≤–æ–ª:
    # —Ç–æ–≥–¥–∞ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –±—É–¥–µ—Ç —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∏–º–≤–æ–ª/—Å—É—Ñ—Ñ–∏–∫—Å
    if i < n - 1:
        return False

    # 2) –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–ª–∏—á–∞—é—â–∏–π—Å—è —Ö–≤–æ—Å—Ç ‚Äî —Ç–∏–ø–∏—á–Ω–æ–µ –ø–∞–¥–µ–∂–Ω–æ–µ –æ–∫–æ–Ω—á–∞–Ω–∏–µ
    bad_ends = (
        "–∞", "—è", "—ã", "–∏", "–µ", "—é", "—É",
        "–æ–π", "–µ–π", "–æ–º", "–µ–º", "–æ—é", "–µ—é"
    )

    def strip_bad_end(s: str) -> str:
        for suf in bad_ends:
            if s.endswith(suf) and len(s) > len(suf) + 1:
                return s[:-len(suf)]
        # –µ—Å–ª–∏ –Ω–µ—Ç "–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ" –æ–∫–æ–Ω—á–∞–Ω–∏—è, –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ —É–±—Ä–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–ª–∞—Å–Ω—É—é
        vowels = "–∞–µ—ë–∏–æ—É—ã—ç—é—è"
        if s[-1] in vowels and len(s) > 3:
            return s[:-1]
        return s

    stem_a = strip_bad_end(a_l)
    stem_b = strip_bad_end(b_l)

    # —Å—Ç–µ–º—ã –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å –∏ –±—ã—Ç—å –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º–∏
    if len(stem_a) < 3 or len(stem_b) < 3:
        return False

    return stem_a == stem_b

def build_clusters(
    mentions: List[Mention],
    nlp,
) -> List[Set[int]]:
    """
    –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–º—ë–Ω –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º mentions:
      - —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ª–µ–º–º,
      - –≤–∫–ª—é—á–µ–Ω–∏–µ –æ–¥–Ω–æ—Å–ª–æ–≤–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –≤ –§–ò–û,
      - –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–∑–≤–∏—â–∞ —Ç–∏–ø–∞ "–ò–≤–∞–Ω—ã—á"~"–ò–≤–∞–Ω".
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–Ω–æ–∂–µ—Å—Ç–≤ –∏–Ω–¥–µ–∫—Å–æ–≤ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π.
    """
    n = len(mentions)
    if n == 0:
        return []

    # DSU
    parent = list(range(n))

    def find(i: int) -> int:
        while parent[i] != i:
            parent[i] = parent[parent[i]]
            i = parent[i]
        return i

    def union(i: int, j: int):
        ri, rj = find(i), find(j)
        if ri != rj:
            parent[rj] = ri

    # –ø—Ä–µ–¥—Ä–∞—Å—á—ë—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º –∏ –ª–µ–º–º
    name_lemmas: List[Set[str]] = []
    tokens: List[List[str]] = []
    for m in mentions:
        names = [m.text for m in mentions]
        name_lemmas.append(set(m.lemmas))
        tokens.append([t.lower() for t in m.text.split()])

    docs = [nlp(m.text) for m in mentions]

    # 1) –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ª–µ–º–º ‚Üí —Å–∫–ª–µ–∏–≤–∞–µ–º
    #    –î–ª—è –∏–º—ë–Ω —Å —Ü–∏—Ñ—Ä–∞–º–∏:
    #      - –ù–ï —Å–∫–ª–µ–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ ('–°–û–¢–†–£–î–ù–ò–ö' vs '–°–û–¢–†–£–î–ù–ò–ö 2'),
    #      - –ù–û —Å–∫–ª–µ–∏–≤–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ ('–ü–û–î–ñ–ò–ì–ê–¢–ï–õ–¨ 2' vs '–ü–û–î–ñ–ò–ì–ê–¢–ï–õ–¨ 2').
    for i in range(n):
        for j in range(i + 1, n):
            if not (name_lemmas[i] and name_lemmas[i] == name_lemmas[j]):
                continue

            ni = names[i].strip().lower()
            nj = names[j].strip().lower()

            if _has_digit(ni) or _has_digit(nj):
                # –µ—Å–ª–∏ –æ–±–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞—é—Ç ‚Äî —Å–∫–ª–µ–∏–≤–∞–µ–º
                if ni == nj:
                    union(i, j)
                # –∏–Ω–∞—á–µ (—Ä–∞–∑–Ω—ã–µ —Ü–∏—Ñ—Ä—ã / –±–∞–∑–∞) ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
                continue

            # –æ–±—ã—á–Ω—ã–µ –∏–º–µ–Ω–∞ –±–µ–∑ —Ü–∏—Ñ—Ä ‚Äî —Å–∫–ª–µ–∏–≤–∞–µ–º –∫–∞–∫ —Ä–∞–Ω—å—à–µ
            union(i, j)


    # 2) –æ–¥–Ω–æ—Å–ª–æ–≤–Ω–æ–µ –∏–º—è ‚äÇ –§–ò–û (–æ–±—â–∞—è –ª–µ–º–º–∞ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞)
    for i in range(n):
        for j in range(i + 1, n):
            if _has_digit(names[i]) or _has_digit(names[j]):
                continue
            toks_i, toks_j = tokens[i], tokens[j]

            # –æ–¥–Ω–æ —Å–ª–æ–≤–æ vs –¥–≤–∞ –∏ –±–æ–ª–µ–µ
            if len(toks_i) == 1 and len(toks_j) >= 2:
                if mentions[i].lemmas and mentions[j].lemmas:
                    if mentions[i].lemmas[0] == mentions[j].lemmas[0]:
                        union(i, j)
            elif len(toks_j) == 1 and len(toks_i) >= 2:
                if mentions[i].lemmas and mentions[j].lemmas:
                    if mentions[j].lemmas[0] == mentions[i].lemmas[0]:
                        union(i, j)

    # 3) –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ –∞–ª–∏–∞—Å—ã: "–ü—å—è–Ω–∞—è –ö–æ—Ç–Ω–∏–∫–æ–≤–∞" ~ "–ö–û–¢–ù–ò–ö–û–í–ê"
    #    –æ–¥–Ω–æ—Å–ª–æ–≤–Ω–æ–µ PROPN <-> –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ–µ, –≥–¥–µ:
    #      - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω ADJ,
    #      - –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω PROPN —Å —Ç–æ–π –∂–µ –ª–µ–º–º–æ–π, —á—Ç–æ –∏ –æ–¥–Ω–æ—Å–ª–æ–≤–Ω–æ–µ –∏–º—è
    for i in range(n):
        for j in range(i + 1, n):
            toks_i, toks_j = tokens[i], tokens[j]
            doc_i,  doc_j  = docs[i],   docs[j]

            # –≤–∞—Ä–∏–∞–Ω—Ç: i ‚Äî –æ–¥–Ω–æ—Å–ª–æ–≤–Ω–æ–µ –∏–º—è, j ‚Äî "–ü—å—è–Ω–∞—è –ö–æ—Ç–Ω–∏–∫–æ–≤–∞"
            if len(toks_i) == 1 and len(toks_j) >= 2:
                if len(doc_i) == 1 and doc_i[0].pos_ == "PROPN":
                    first_j = doc_j[0]
                    last_j  = doc_j[-1]
                    if (
                        first_j.pos_ == "ADJ"
                        and last_j.pos_ == "PROPN"
                        and last_j.lemma_.lower() == doc_i[0].lemma_.lower()
                    ):
                        union(i, j)
                        continue

            # —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: j ‚Äî –æ–¥–Ω–æ—Å–ª–æ–≤–Ω–æ–µ –∏–º—è, i ‚Äî "–ü—å—è–Ω–∞—è –ö–æ—Ç–Ω–∏–∫–æ–≤–∞"
            if len(toks_j) == 1 and len(toks_i) >= 2:
                if len(doc_j) == 1 and doc_j[0].pos_ == "PROPN":
                    first_i = doc_i[0]
                    last_i  = doc_i[-1]
                    if (
                        first_i.pos_ == "ADJ"
                        and last_i.pos_ == "PROPN"
                        and last_i.lemma_.lower() == doc_j[0].lemma_.lower()
                    ):
                        union(i, j)
                        continue


    # 3) –ø—Ä–æ–∑–≤–∏—â–∞ '–ò–≤–∞–Ω—ã—á' ~ '–ò–≤–∞–Ω'
    for i in range(n):
        for j in range(i + 1, n):
            t_i = tokens[i]
            t_j = tokens[j]
            if len(t_i) == 1 and len(t_j) == 1:
                a, b = t_i[0], t_j[0]
                if _nickname_matches_base(a, b) or _nickname_matches_base(b, a):
                    union(i, j)

    # 4) –ø–∞–¥–µ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ –∏–º–µ–Ω–∏: –ü—Ä–æ—Ö–æ—Ä ~ –ü—Ä–æ—Ö–æ—Ä–∞
    for i in range(n):
        for j in range(i + 1, n):
            t_i = tokens[i]
            t_j = tokens[j]
            if len(t_i) == 1 and len(t_j) == 1:
                if _same_name_case_variant(t_i[0], t_j[0]):
                    union(i, j)

    # —Å–æ–±–∏—Ä–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
    clusters_map: Dict[int, Set[int]] = {}
    for i in range(n):
        r = find(i)
        clusters_map.setdefault(r, set()).add(i)

    return list(clusters_map.values())

import re

def choose_canonical_for_cluster(
    cluster: list[int],
    mentions: list,
    freq: dict[str, int],
) -> tuple[str, bool, str, float]:
    """
    –í—ã–±–∏—Ä–∞–µ–º –∫–∞–Ω–æ–Ω –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞.

    –õ–û–ì–ò–ö–ê:
      1) –ö–∞–Ω–æ–Ω –±–µ—Ä—ë–º –¢–û–õ–¨–ö–û –∏–∑ —è–∫–æ—Ä–µ–π (–ö–ê–ü–°-—Å–ø–∏–∫–µ—Ä—ã –∏ —à–∞–ø–∫–∏), –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å.
      2) –ï—Å–ª–∏ —è–∫–æ—Ä–µ–π –Ω–µ—Ç ‚Äî fallback: –≤—ã–±–∏—Ä–∞–µ–º –∏–∑ –ª—é–±—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä–∞.
      3) –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–Ω–æ–Ω–æ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–º—ë–Ω —Å—Ä–∞–∑—É:
         '–ì–ï–ù–ê, –ö–ê–¢–Ø', '–ì–ï–ù–´ –ò –ö–ê–¢–ò' –∏ —Ç.–ø.
      4) –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: dialog > header > prim > ner,
         –ø–æ—Ç–æ–º —á–∞—Å—Ç–æ—Ç–∞, –ø–æ—Ç–æ–º ¬´–ø—Ä–æ—Å—Ç–æ—Ç–∞¬ª —Ñ–æ—Ä–º—ã.
    """

    if not cluster:
        return "", False, "", 0.0

    # --- –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

    def is_multi_person_name(name: str) -> bool:
        """
        –ü—Ä–∏–∑–Ω–∞–∫–∏ "–º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ" –∏–º–µ–Ω–∏:
          - –µ—Å—Ç—å '–∏' –º–µ–∂–¥—É –¥–≤—É–º—è Capitalized —Å–ª–æ–≤–∞–º–∏,
          - –∏–ª–∏ –µ—Å—Ç—å –∑–∞–ø—è—Ç–∞—è –∏ –∫–∞–∫ –º–∏–Ω–∏–º—É–º –¥–≤–∞ —Å–ª–æ–≤–∞ –≤–∏–¥–∞ [–ê-–Ø–Å][–∞-—è—ë]+.
        """
        # –¥–≤–∞ —Å–ª–æ–≤–∞ "–ò–º—è –∏ –ò–º—è"
        if re.search(r"\b[–ê-–Ø–Å][–∞-—è—ë]+\b\s+–∏\s+\b[–ê-–Ø–Å][–∞-—è—ë]+\b", name):
            return True
        # –∏–ª–∏ "–ò–º—è, –ò–º—è"
        caps_words = re.findall(r"\b[–ê-–Ø–Å][–∞-—è—ë]+\b", name)
        if "," in name and len(caps_words) >= 2:
            return True
        return False

    def kind_rank(k: str) -> int:
        # —á–µ–º –±–æ–ª—å—à–µ ‚Äî —Ç–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ
        return {
            "dialog": 3,
            "header": 2,
            "prim":   1,
            "ner":    0,
        }.get(k, 0)

    def score(idx: int) -> tuple:
        m = mentions[idx]
        name = m.text or ""
        # –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π —Ñ–ª–∞–≥ –¥–ª—è –º–Ω–æ–≥–æ–ª—é–¥–Ω—ã—Ö –∏–º—ë–Ω
        multi = is_multi_person_name(name)
        # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ "—Å–ª–æ–≤" –≤ –∏–º–µ–Ω–∏
        tokens = re.findall(r"\b[A-Za-z–ê-–Ø–∞-—è–Å—ë]+\b", name)
        return (
            kind_rank(getattr(m, "kind", "")),  # 1) dialog > header > ...
            0 if multi else 1,                  # 2) –æ–¥–∏–Ω–æ—á–Ω–æ–µ –∏–º—è –ª—É—á—à–µ –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–≥–æ
            freq.get(name, 1),                  # 3) —á–∞—â–µ –≤—Å—Ç—Ä–µ—á–∞—é—â–µ–µ—Å—è –ª—É—á—à–µ
            -len(tokens),                       # 4) –º–µ–Ω—å—à–µ —Å–ª–æ–≤ ‚Üí –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ
            -len(name),                         # 5) –∫–æ—Ä–æ—á–µ —Å—Ç—Ä–æ–∫–∞ ‚Üí –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ
            -getattr(m, "line_idx", 0),         # 6) —Ä–∞–Ω—å—à–µ –≤ —Ç–µ–∫—Å—Ç–µ ‚Üí —á—É—Ç—å –ª—É—á—à–µ
        )

    # --- 1) –¥–µ–ª–∏–º mentions –Ω–∞ —è–∫–æ—Ä–Ω—ã–µ –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ---

    anchor_idxs = [
        i for i in cluster
        if getattr(mentions[i], "is_anchor", False)
        and getattr(mentions[i], "kind", "") in ("dialog", "header")
    ]

    non_anchor_idxs = [i for i in cluster if i not in anchor_idxs]

    # --- 2) –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –∫–∞–Ω–æ–Ω–∞ ---

    candidates = anchor_idxs or non_anchor_idxs

    # –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É–±–∏—Ä–∞–µ–º "–ì–ï–ù–ê, –ö–ê–¢–Ø" –∏ –ø—Ä.
    single_person_candidates = [
        i for i in candidates if not is_multi_person_name(mentions[i].text or "")
    ]
    if single_person_candidates:
        candidates = single_person_candidates

    # --- 3) –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –ø–æ score ---

    best_idx = max(candidates, key=score)
    best_mention = mentions[best_idx]
    canonical = best_mention.text or ""

    # --- 4) –º–µ—Ç–∫–∞ is_main: –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Å–ø–∏–∫–µ—Ä –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ ---
    is_main = any(
        getattr(mentions[i], "kind", "") == "dialog"
        for i in cluster
    )

    # --- 5) source: –∏–∑ –∫–∞–∫–∏—Ö —Ç–∏–ø–æ–≤ —è–∫–æ—Ä–µ–π –ø—Ä–∏—à–ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ ---
    src_bits = set()
    for i in cluster:
        m = mentions[i]
        if getattr(m, "is_anchor", False):
            src_bits.add(getattr(m, "kind", ""))
    source = "+".join(sorted(src_bits)) if src_bits else "other"

    # --- 6) confidence: –ø—Ä–∏–º–∏—Ç–∏–≤–Ω–æ, –Ω–æ —á–µ—Å—Ç–Ω–æ ---
    if anchor_idxs:
        conf = 1.0   # –µ—Å—Ç—å —è–∫–æ—Ä—è ‚Üí —É–≤–µ—Ä–µ–Ω—ã –≤ –∫–∞–Ω–æ–Ω–µ
    else:
        conf = 0.7   # —Ç–æ–ª—å–∫–æ –Ω–µ—è–∫–æ—Ä–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è

    return canonical, is_main, source, conf


# -------------------- –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∏–∑–≤–ª–µ—á—å –≥–µ—Ä–æ–µ–≤ —Å—Ü–µ–Ω—ã --------------------
def cleanup_char_name(name: str) -> str | None:
    """
    –ß–∏—Å—Ç–∏—Ç —Å—ã—Ä–æ–µ –∏–º—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –∏–∑ —à–∞–ø–æ–∫/—Å–ø–∏—Å–∫–æ–≤:
    - –æ–±—Ä–µ–∑–∞–µ—Ç –ó/–ö / –ó–ö,
    - —É–±–∏—Ä–∞–µ—Ç —Å–∫–æ–±–∫–∏,
    - –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç '–ö–û–ù–ï–¶ –ü–ï–†–í–û–ô –°–ï–†–ò–ò' –∏ –ø–æ–¥–æ–±–Ω–æ–µ,
    - –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç–∏–ø–∞ '–ë–¨–ï–¢ –¢–ò–ú–£–†–ê'.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - –æ—á–∏—â–µ–Ω–Ω–æ–µ –∏–º—è, –∏–ª–∏
      - None, –µ—Å–ª–∏ —ç—Ç–æ –º—É—Å–æ—Ä.
    """
    if not name:
        return None

    s = name.strip()

    # —É–±–∏—Ä–∞–µ–º –≤—Å—ë –≤ —Å–∫–æ–±–∫–∞—Ö: '–†–ò–¢–ê (–í–°–ï–ú' -> '–†–ò–¢–ê'
    s = re.sub(r"\(.*", "", s).strip()

    # –æ–±—Ä–µ–∑–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –ó/–ö / –ó–ö (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞/–ª–∞—Ç–∏–Ω–∏—Ü–∞) –∏ —Ö–≤–æ—Å—Ç –ø–æ—Å–ª–µ –Ω–µ–≥–æ:
    # '–ú–ò–®–ê –ó/–ö' -> '–ú–ò–®–ê', '–ü–ê–†–ï–ù–¨ 1 –ó/–ö' -> '–ü–ê–†–ï–ù–¨ 1'
    s = re.sub(r"\s+–ó\s*/?\s*[–öK]\b.*", "", s, flags=re.IGNORECASE).strip()

    if not s:
        return None

    low = s.lower()

    # —è–≤–Ω—ã–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ ‚Äî '–ö–û–ù–ï–¶ –ü–ï–†–í–û–ô –°–ï–†–ò–ò', '–ö–û–ù–ï–¶ –°–ï–†–ò–ò', '–ö–û–ù–ï–¶ –°–ï–ó–û–ù–ê'
    if ("–∫–æ–Ω–µ—Ü" in low and "—Å–µ—Ä–∏" in low) or "—Å–µ–∑–æ–Ω" in low:
        return None

    # –≥—Ä—É–±–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ç–∏–≤ '–ë–¨–ï–¢ –¢–ò–ú–£–†–ê' –∏ —Ç.–ø.:
    parts = s.split()
    if len(parts) >= 2:
        first = parts[0].lower()
        # –µ—Å–ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –≥–ª–∞–≥–æ–ª (–±—å–µ—Ç, –∏–¥–µ—Ç, –≥–æ–≤–æ—Ä–∏—Ç, —Å—Ç—Ä–µ–ª—è–µ—Ç...)
        if re.search(r"(–µ—Ç|—ë—Ç|–∏—Ç|–∞–ª|–∞–ª–∞|–∞–ª–∏|—é—Ç|—É—Ç|–µ—à—å|–∞–µ—à—å|–∞–µ—Ç|—è–µ—Ç)$", first):
            return None

    # –ø–æ—Å–ª–µ –≤—Å–µ—Ö —á–∏—Å—Ç–æ–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å
    return s if s.strip() else None

def extract_scene_characters(
    scene_text: str,
    nlp=None,
) -> List[Character]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–¥–Ω–æ–π —Å—Ü–µ–Ω—ã.
    –ù–∞ –≤—Ö–æ–¥: —Ç–µ–∫—Å—Ç —Å—Ü–µ–Ω—ã (—Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, –æ–ø–∏—Å–∞–Ω–∏–µ–º, –¥–∏–∞–ª–æ–≥–∞–º–∏).
    –ù–∞ –≤—ã—Ö–æ–¥: —Å–ø–∏—Å–æ–∫ Character —Å –∫–∞–Ω–æ–Ω–∞–º–∏ –∏ –∞–ª–∏–∞—Å–∞–º–∏.
    """
    nlp = nlp or load_ru()

    def _to_nom_caps(name: str) -> str:
        """
        –ü—Ä–∏–≤–æ–¥–∏—Ç –∏–º—è –∫ —á–µ–º—É-—Ç–æ –ø–æ—Ö–æ–∂–µ–º—É –Ω–∞ –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂ –∏ –ö–ê–ü–°:
          - –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ—Ä—ë–º –ª–µ–º–º—É spaCy,
          - –µ—Å–ª–∏ —Å–ª–æ–≤–æ —Å—Ç–æ–∏—Ç –ù–ï –≤ –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º –ø–∞–¥–µ–∂–µ,
            –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ ¬´–∫–æ—Å–≤–µ–Ω–Ω—ã–µ¬ª —Ñ–æ—Ä–º—ã –∏–º—ë–Ω:
              –°–∞—à–µ–π ‚Üí –°–∞—à–∞, –ì–µ–Ω–æ–π ‚Üí –ì–µ–Ω–∞, –†–∏—Ç—É ‚Üí –†–∏—Ç–∞, –ö–∞—Ç–∏ ‚Üí –ö–∞—Ç—è,
          - –≤—Å—ë –ø—Ä–∏–≤–æ–¥–∏–º –∫ –í–ï–†–•–ù–ï–ú–£ –†–ï–ì–ò–°–¢–†–£.
        """
        if not name:
            return ""

        doc_name = nlp(name)
        parts = []

        for t in doc_name:
            if not t.is_alpha:
                parts.append(t.text)
                continue

            surf = t.text
            surf_low = surf.lower()
            lemma_low = t.lemma_.lower()

            # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ—Ä—ë–º –ª–µ–º–º—É
            base = lemma_low

            # —Å–º–æ—Ç—Ä–∏–º –ø–∞–¥–µ–∂
            cases = t.morph.get("Case")
            is_nom = "Nom" in cases  # True, –µ—Å–ª–∏ –µ—Å—Ç—å –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π

            # —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–ª–æ–≤–æ –ù–ï –≤ –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º –ø–∞–¥–µ–∂–µ,
            # –ø—ã—Ç–∞–µ–º—Å—è ¬´–æ—Ç–∫–∞—Ç–∏—Ç—å¬ª —Ç–∏–ø–∏—á–Ω—ã–µ –∫–æ—Å–≤–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã
            if not is_nom:
                # –¢–∏–ø–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—ã —Ç–∏–ø–∞ "–°–∞—à–µ–π", "–ì–µ–Ω–æ–π" ‚Üí "–°–∞—à–∞", "–ì–µ–Ω–∞"
                if len(surf_low) > 3 and surf_low.endswith(("–æ–π", "–µ–π", "—ë–π")):
                    base = surf_low[:-2] + "–∞"
                # –§–æ—Ä–º—ã –Ω–∞ "—É/—é": "–†–∏—Ç—É", "–ö–∞—Ç—é" ‚Üí "–†–∏—Ç–∞", "–ö–∞—Ç—è"
                elif len(surf_low) > 3 and surf_low.endswith(("—É", "—é")):
                    base = surf_low[:-1] + "–∞"
                # –ì–µ–Ω–∏—Ç–∏–≤/–¥–∞—Ç–µ–ª—å–Ω—ã–π "–ö–∞—Ç–∏" ‚Üí "–ö–∞—Ç—è"
                elif len(surf_low) > 3 and surf_low.endswith("–∏"):
                    base = surf_low[:-1] + "—è"

            parts.append(base)

        norm = " ".join(parts).strip()
        norm = norm.replace("—ë", "–µ").replace("–Å", "–ï")
        return norm.upper()


    # 1) –ö–ê–ü–°-—Å–ø–∏–∫–µ—Ä—ã
    dialog_raw = extract_dialog_speakers(scene_text, nlp)
    # 2) –∏–º–µ–Ω–∞ –∏–∑ —à–∞–ø–∫–∏
    header_raw = extract_header_names(scene_text)

    prim_raw = extract_prim_names(scene_text)

    # 3) NER, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–π –∫ —è–∫–æ—Ä—è–º
    anchor_names = [name for (name, *_rest) in dialog_raw] + [
        name for (name, *_rest) in header_raw
    ]
    ner_raw = extract_ner_persons(scene_text, nlp, anchor_names)

    # 4) –°–æ–±–∏—Ä–∞–µ–º mentions
    mentions: List[Mention] = []

    # –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–æ: —Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Ñ–æ—Ä–º
    freq: Dict[str, int] = {}

    def add_mention(name: str, kind: str, line_idx: int, start: int, end: int, is_anchor: bool):
        # 0) —Å–Ω–∞—á–∞–ª–∞ —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ–º —Å—ã—Ä–æ–µ –∏–º—è
        base = cleanup_char_name(name)
        if not base:
            return

        # 1) –¥–∞–ª—å—à–µ —Ç–≤–æ—è –æ–±—ã—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        name_clean = _clean_person_name(base)
        if not name_clean:
            return

        doc = nlp(name_clean)
        lemmas = tuple(t.lemma_.lower() for t in doc if t.is_alpha)
        if not lemmas:
            return

        m = Mention(
            text=name_clean,
            kind=kind,
            span=(start, end),
            line_idx=line_idx,
            lemmas=lemmas,
            is_anchor=is_anchor,
        )
        mentions.append(m)
        freq[name_clean] = freq.get(name_clean, 0) + 1


    for name, line_idx, start, end in dialog_raw:
        add_mention(name, "dialog", line_idx, start, end, True)

    for name, line_idx, start, end in header_raw:
        add_mention(name, "header", line_idx, start, end, True)

    for name, line_idx, start, end in prim_raw:
        add_mention(name, "prim", line_idx, start, end, False)

    for name, line_idx, start, end in ner_raw:
        add_mention(name, "ner", line_idx, start, end, False)

    if not mentions:
        return []

    # 5) –ö–ª–∞—Å—Ç–µ—Ä—ã –∫–æ-—Ä–µ—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –ø–æ –∏–º–µ–Ω–∞–º
    clusters = build_clusters(mentions, nlp)

    # 6) –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤—ã–±–∏—Ä–∞–µ–º –∫–∞–Ω–æ–Ω –∏ –∞–ª–∏–∞—Å—ã
    characters: List[Character] = []
    for cid, cluster in enumerate(clusters):
        canonical, is_main, src, conf = choose_canonical_for_cluster(
            cluster, mentions, freq
        )
        aliases = {mentions[i].text for i in cluster if mentions[i].text != canonical}
        characters.append(
            Character(
                id=cid,
                canonical_name=canonical,
                aliases=aliases,
                is_main=is_main,
                source=src,
                confidence=conf,
            )
        )

    # --- –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø: –ò–ú–ï–ù–ò–¢–ï–õ–¨–ù–´–ô –ü–ê–î–ï–ñ + –ö–ê–ü–° ---
    for ch in characters:
        if ch.canonical_name:
            ch.canonical_name = _to_nom_caps(ch.canonical_name)
        else:
            ch.canonical_name = ""

        if ch.aliases:
            ch.aliases = {_to_nom_caps(a) for a in ch.aliases}
        else:
            ch.aliases = set()

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ "–≤–∞–∂–Ω–æ—Å—Ç–∏" (–¥–∏–∞–ª–æ–≥–æ–≤—ã–µ –≤–ø–µ—Ä—ë–¥)
    characters.sort(key=lambda c: (not c.is_main, -c.confidence, c.canonical_name))

    return characters
    
def extract_scene_entities(
    scene_text: str,
    object_: str = "",
    subobject: str = "",
    nlp=None,
):
    """
    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:
      - characters: —Å–ø–∏—Å–æ–∫ Character (–≥–µ—Ä–æ–∏ —Å –∫–∞–Ω–æ–Ω–æ–º –∏ –∞–ª–∏–∞—Å–∞–º–∏),
      - grouping:   —Ç–µ—Ö–Ω–∞—Ä—Å–∫–∏–µ/–≥—Ä—É–ø–ø–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–¢–µ—Ö–Ω–∏–∫–∏ (2)'),
      - massovka:   –º–∞—Å—Å–æ–≤–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ß—É–∫—á–∏ (10)', '–¢–æ–ª–ø–∞').
    """
    nlp = nlp or load_ru()

    # 1) –ø–µ—Ä—Å–æ–Ω–∞–∂–∏
    characters = extract_scene_characters(scene_text, nlp)

    try:
        main_chars = [c for c in characters if getattr(c, "is_main", True)]
        n_chars = len(main_chars)
    except Exception:
        n_chars = len(characters)

    # 2) —è–≤–Ω–∞—è –º–∞—Å—Å–æ–≤–∫–∞ –∏ –≥—Ä—É–ø–ø–æ–≤–∫–∞
    massovka_explicit, grouping = _extract_massovka_and_grouping(scene_text)

    # 3) —Å–∫—Ä—ã—Ç–∞—è –º–∞—Å—Å–æ–≤–∫–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Ñ–æ–Ω (—Ç–æ–ª–ø–∞, –ª—é–¥–∏, —Ç—É—Ä–∏—Å—Ç—ã...) –≤ –ø—É–±–ª–∏—á–Ω—ã—Ö –ª–æ–∫–∞—Ü–∏—è—Ö
    implicit_mass = extract_implicit_massovka(
        scene_text,
        object_=object_,
        subobject=subobject,
        nlp=nlp,
        n_chars=n_chars,
        min_chars=3,
    )

    massovka_all = sorted(set(massovka_explicit) | implicit_mass)

    small_groups = _extract_small_numeric_grouping(scene_text)
    grouping = set(grouping) | small_groups

    return {
        "characters": characters,
        "grouping": sorted(grouping),
        "massovka": massovka_all,
    }

MODEL_DIR = resource_path("ner_rubert_best_1763076515/ner_rubert_best")

# ==== 3. –ü–∞—Ä—Å–µ—Ä —Å–ª–µ–¥—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ù–ù ====
ner_pipe = pipeline(
    "token-classification",
    model=str(MODEL_DIR),
    tokenizer=str(MODEL_DIR),
    aggregation_strategy=None,
    device=0
    #device=get_device()
)

# === 1Ô∏è‚É£ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
#MODEL_PATH = r"D:\WINK_1\ner_rubert_best_1763076515\ner_rubert_best"
MODEL_PATH = MODEL_DIR

entity_cols = [
    '–ì—Ä–∏–º', '–ö–æ—Å—Ç—é–º', '–†–µ–∫–≤–∏–∑–∏—Ç', '–î–µ–∫–æ—Ä–∞—Ü–∏—è',
    '–ü–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞', '–ö–∞—Å–∫–∞–¥–µ—Ä', '–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç',
]

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
device = 'cuda' if torch.cuda.is_available() else 'cpu'


# === 2Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ===
def extract_entities_from_text(text, model, threshold=0.2, max_length=512):
    # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å "–æ–∫–æ–Ω—Ü–∞–º–∏"
    inputs = tokenizer(
        text,
        return_overflowing_tokens=True,
        stride=50,
        max_length=max_length,
        truncation=True,
        return_offsets_mapping=True
    )

    all_results = []

    special_tokens = {"[CLS]", "[SEP]", "[PAD]"}

    for i in range(len(inputs["input_ids"])):
        input_ids      = torch.tensor([inputs["input_ids"][i]]).to(device)
        attention_mask = torch.tensor([inputs["attention_mask"][i]]).to(device)
        offsets        = inputs["offset_mapping"][i]   # <<< –≤–∞–∂–Ω–æ–µ

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            logits  = outputs.logits
            probs   = torch.softmax(logits, dim=-1)
            scores, preds = torch.max(probs, dim=-1)

        tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][i])

        # —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
        current_entity = None
        span_start     = None
        span_end       = None
        current_scores = []

        for token, offset, pred_id, score in zip(
            tokens,
            offsets,
            preds[0].cpu().numpy(),
            scores[0].cpu().numpy()
        ):
            start_char, end_char = offset

            # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü-—Ç–æ–∫–µ–Ω—ã –∏ –ø–æ–∑–∏—Ü–∏–∏ –±–µ–∑ –æ—Ñ—Ñ—Å–µ—Ç–∞
            if token in special_tokens or (start_char == 0 and end_char == 0):
                continue

            label = model.config.id2label[pred_id]
            label = label.replace("B-", "").replace("I-", "")

            # –Ω–µ —Å—É—â–Ω–æ—Å—Ç—å ‚Üí –∑–∞–∫—Ä—ã–≤–∞–µ–º, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ —à–ª–æ
            if label == "O":
                if current_entity is not None and span_start is not None and span_end is not None:
                    span_text = text[span_start:span_end].strip()
                    if len(span_text) > 1:
                        avg_score = round(sum(current_scores) / len(current_scores), 3)
                        all_results.append({
                            "entity": current_entity,
                            "word": span_text,
                            "score": avg_score,
                        })
                # —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                current_entity = None
                span_start     = None
                span_end       = None
                current_scores = []
                continue

            # –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è —Ç–∞ –∂–µ —Å—É—â–Ω–æ—Å—Ç—å
            if label == current_entity:
                # —Ä–∞—Å—à–∏—Ä—è–µ–º –ø—Ä–∞–≤—É—é –≥—Ä–∞–Ω–∏—Ü—É
                span_end = end_char
                current_scores.append(score)
            else:
                # –Ω–∞—á–∞–ª–∞—Å—å –Ω–æ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å ‚Üí –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä—É—é
                if current_entity is not None and span_start is not None and span_end is not None:
                    span_text = text[span_start:span_end].strip()
                    if len(span_text) > 1:
                        avg_score = round(sum(current_scores) / len(current_scores), 3)
                        all_results.append({
                            "entity": current_entity,
                            "word": span_text,
                            "score": avg_score,
                        })
                # –æ—Ç–∫—Ä—ã–≤–∞–µ–º –Ω–æ–≤—É—é
                current_entity = label
                span_start     = start_char
                span_end       = end_char
                current_scores = [score]

        # –∑–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—É—â–Ω–æ—Å—Ç—å –≤ —á–∞–Ω–∫–µ
        if current_entity is not None and span_start is not None and span_end is not None:
            span_text = text[span_start:span_end].strip()
            if len(span_text) > 1:
                avg_score = round(sum(current_scores) / len(current_scores), 3)
                all_results.append({
                    "entity": current_entity,
                    "word": span_text,
                    "score": avg_score,
                })

    # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ (–ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –æ–∫–Ω–∞ –∏ —Ç.–ø.)
    df = pd.DataFrame(all_results)
    if not df.empty:
        df = df.drop_duplicates(subset=["entity", "word"])

    return df.to_dict("records")


DIMINUTIVE_SUFFIXES = ("–∏–∫", "—á–∏–∫", "—â–∏–∫", "–æ–∫", "–µ–∫", "–µ—á–µ–∫", "—É—à–∫", "—é—à–∫", "–∏—à–∫")

def _lemma_base(lemma: str) -> str:
    """
    –°—Ç—Ä–æ–∏–º –±–æ–ª–µ–µ ¬´–≥—Ä—É–±—É—é¬ª –±–∞–∑—É –ª–µ–º–º—ã –¥–ª—è —Å–∫–ª–µ–π–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤:
      —Ñ–æ–Ω–∞—Ä–∏–∫ / —Ñ–æ–Ω–∞—Ä—å ‚Üí —Ñ–æ–Ω–∞—Ä
      –Ω–æ–∂ / –Ω–æ–∂–∏–∫ ‚Üí –Ω–æ–∂  (–ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –ù–ï —Å–∫–ª–µ–∏–≤–∞—Ç—å, –µ—Å–ª–∏ —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ)
    """
    s = lemma.lower().replace("—ë", "–µ")
    # —É–±–∏—Ä–∞–µ–º –∫–æ–Ω–µ—á–Ω—ã–π –º—è–≥–∫–∏–π –∑–Ω–∞–∫
    if s.endswith("—å"):
        s = s[:-1]
    # —Å–Ω–∏–º–∞–µ–º —É–º–µ–Ω—å—à–∏—Ç–µ–ª—å–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã
    for suf in DIMINUTIVE_SUFFIXES:
        if s.endswith(suf) and len(s) > len(suf) + 1:
            s = s[: -len(suf)]
            break
    return s

def clean_requisite_entities(raw_ents, nlp, min_score: float = 0.4):
    """
    raw_ents: —Å–ø–∏—Å–æ–∫ dict'–æ–≤ –≤–∏–¥–∞ {"entity","word","score"} –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ –†–ï–ö–í–ò–ó–ò–¢/–û–ë–™–ï–ö–¢/–ü–û–î–û–ë–™–ï–ö–¢.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π:
        {
          "lemma":   "–§–æ–Ω–∞—Ä–∏–∫",           # –∫–∞–Ω–æ–Ω –ø–æ –≥—Ä—É–ø–ø–µ
          "surface": ["—Ñ–æ–Ω–∞—Ä–∏–∫–∞", ...],  # —Ñ–æ—Ä–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
          "score":   0.849               # max –ø–æ –≥—Ä—É–ø–ø–µ
        }
    """
    buckets = []
    vowels = set("–∞–µ—ë–∏–æ—É—ã—ç—é—è")

    for ent in raw_ents:
        score = float(ent.get("score", 0.0))
        if score < min_score:
            continue

        phrase = (ent.get("word") or "").strip()
        if len(phrase) < 2:
            continue

        # —Ç–æ–ª—å–∫–æ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + –ø—Ä–æ–±–µ–ª—ã/–¥–µ—Ñ–∏—Å—ã
        if not re.search(r"[–ê-–Ø–∞-—è–Å—ë]", phrase):
            continue

        # ---- 0) —Ñ–∏–ª—å—Ç—Ä —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –º—É—Å–æ—Ä–∞ (–®–∫–∏ –∏ –ø–æ–¥–æ–±–Ω–æ–µ) ----
        if len(phrase) <= 3:
            doc_short = nlp(phrase)
            toks_short = [t for t in doc_short if t.is_alpha]
            # –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º
            if not toks_short:
                continue
            # –µ—Å–ª–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã OOV –∏ —Å–ª–æ–≤–æ –∫–æ—Ä–æ—Ç–∫–æ–µ ‚Äî –ø–æ—á—Ç–∏ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ —à—É–º
            if all(t.is_oov for t in toks_short):
                continue

        doc = nlp(phrase)
        tokens = [t for t in doc if t.is_alpha]
        if not tokens:
            continue

        # –∏—â–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ / –∏–º–µ–Ω–∞
        nouns = [t for t in tokens if t.pos_ in {"NOUN", "PROPN"}]
        if not nouns:
            continue

        head = nouns[-1]
        lemma_raw = ru_lemma(head.text).replace("—ë", "–µ")

        # –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–æ–≤—Å–µ–º –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö –ª–µ–º–º
        if len(lemma_raw) < 3 or not any(ch in vowels for ch in lemma_raw):
            continue

        canon = lemma_raw[:1].upper() + lemma_raw[1:]

        buckets.append(
            {
                "lemma": canon,        # "–§–æ–Ω–∞—Ä–∏–∫", "–§–æ–Ω–∞—Ä—å", "–ë–∞–Ω–∫"
                "lemma_raw": lemma_raw,  # "—Ñ–æ–Ω–∞—Ä–∏–∫"/"—Ñ–æ–Ω–∞—Ä—å"/"–±–∞–Ω–∫"
                "surface": phrase,     # –∫–∞–∫ –≤ —Ç–µ–∫—Å—Ç–µ
                "score": score,
            }
        )

    if not buckets:
        return []

    # 2) –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ ¬´–±–∞–∑–µ¬ª –ª–µ–º–º—ã, —á—Ç–æ–±—ã —Å–∫–ª–µ–∏–≤–∞—Ç—å –§–æ–Ω–∞—Ä–∏–∫/–§–æ–Ω–∞—Ä—å/–§–æ–Ω–∞—Ä–∏–∫–∞
    cluster_map = defaultdict(list)
    for item in buckets:
        base = _lemma_base(item["lemma_raw"])  # –Ω–∞–ø—Ä–∏–º–µ—Ä, "—Ñ–æ–Ω–∞—Ä"
        cluster_map[base].append(item)

    cleaned = []
    for base, items in cluster_map.items():
        # –∫–∞–Ω–æ–Ω ‚Äî –ø–æ –Ω–∞–∏–±–æ–ª—å—à–µ–º—É score
        best = max(items, key=lambda x: x["score"])
        canon = best["lemma"]

        surfaces = []
        scores   = []
        for it in items:
            if it["surface"] not in surfaces:
                surfaces.append(it["surface"])
            scores.append(it["score"])

        cleaned.append(
            {
                "lemma": canon,                 # —á—Ç–æ –ø–æ–∫–∞–∂–µ–º –≤ —Ç–∞–±–ª–∏—Ü–µ
                "surface": surfaces,            # –∫–∞–∫–∏–µ —Ñ–æ—Ä–º—ã –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å
                "score": round(max(scores), 3),
            }
        )

    cleaned.sort(key=lambda x: -x["score"])
    return cleaned

# === 3Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame ===
def process_dataframe(df, model, nlp, text_col="text", threshold=0.5):
    """
    df        ‚Äî –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å–æ —Å—Ü–µ–Ω–∞–º–∏
    model     ‚Äî —Ç–≤–æ—è NER-–º–æ–¥–µ–ª—å
    nlp       ‚Äî spaCy (load_ru())
    text_col  ‚Äî –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º —Å—Ü–µ–Ω—ã
    threshold ‚Äî –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è –∑–∞–ø–∏—Å–∏ —Å—É—â–Ω–æ—Å—Ç–∏
    """
    REKV_LABELS = {"–†–ï–ö–í–ò–ó–ò–¢"}  # –∫–∞–∫ –≤ id2label –ø–æ—Å–ª–µ –æ–±—Ä–µ–∑–∫–∏ B-/I-

    results = []

    for _, row in df.iterrows():
        text = str(row[text_col])

        # 1) –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        ents = extract_entities_from_text(text, model, threshold=threshold)

        # 2) –¥–µ–ª–∏–º –Ω–∞ —Ä–µ–∫–≤–∏–∑–∏—Ç –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        req_ents   = [e for e in ents if e["entity"] in REKV_LABELS]
        other_ents = [e for e in ents if e["entity"] not in REKV_LABELS]

        # 3) –∑–∞–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫–∏: –≤—Å–µ NN-–∫–æ–ª–æ–Ω–∫–∏ + text
        row_result = {col: "" for col in entity_cols}
        row_result["text"] = text

        # 4) –∑–∞–ø–æ–ª–Ω—è–µ–º –í–°–ï –ö–û–õ–û–ù–ö–ò, –∫—Ä–æ–º–µ "–†–µ–∫–≤–∏–∑–∏—Ç", –∫–∞–∫ —Ä–∞–Ω—å—à–µ
        for ent in other_ents:
            col = ent["entity"]
            for target_col in entity_cols:
                # "–†–µ–∫–≤–∏–∑–∏—Ç" –ø—Ä–æ–ø—É—Å–∫–∞–µ–º, –æ–Ω –±—É–¥–µ—Ç –Ω–∏–∂–µ
                if target_col == "–†–µ–∫–≤–∏–∑–∏—Ç":
                    continue

                if col.lower() in target_col.lower().replace(".", "").replace("_", " "):
                    if row_result[target_col]:
                        row_result[target_col] += ", "
                    row_result[target_col] += f"{ent['word']} ({ent['score']:.3f})"
                    break

        # 5) —Ç–µ–ø–µ—Ä—å –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–º–µ–Ω–Ω–æ —Ä–µ–∫–≤–∏–∑–∏—Ç
        clean_rekv = clean_requisite_entities(
            req_ents,
            nlp=nlp,
            min_score=threshold,
        )

        if clean_rekv:
            # –º–æ–∂–µ—à—å –º–µ–Ω—è—Ç—å —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ –∫–∞–∫ —Ç–µ–±–µ —É–¥–æ–±–Ω–æ
            row_result["–†–µ–∫–≤–∏–∑–∏—Ç"] = "; ".join(
                f"{item['lemma']}"
                for item in clean_rekv
            )
        else:
            row_result["–†–µ–∫–≤–∏–∑–∏—Ç"] = ""

        results.append(row_result)

    return pd.DataFrame(results)

model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH).to(device)


def extract_game_transport_for_scene(
    object_: str,
    subobject: str,
    text: str,
    nlp,
) -> list[str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ (–ú–ê–®–ò–ù–ê, –ê–í–¢–û–ë–£–°, ...)
    –ø–æ –¥–∞–Ω–Ω—ã–º —Å—Ü–µ–Ω—ã.
    –ò—Å—Ç–æ—á–Ω–∏–∫–∏:
      - object / subobject (—à–∞–ø–∫–∞),
      - —Ç–µ–∫—Å—Ç —Å—Ü–µ–Ω—ã (–æ–ø–∏—Å–∞–Ω–∏—è / —Ä–µ–º–∞—Ä–∫–∏ / –¥–∏–∞–ª–æ–≥–∏).
    """
    found: set[str] = set()

    def scan_chunk(chunk: str):
        if not chunk:
            return
        doc = nlp(chunk)
        for tok in doc:
            if not tok.is_alpha:
                continue
            lemma = tok.lemma_.lower()
            for canon, lemmas in TRANSPORT_MAP.items():
                if lemma in lemmas:
                    found.add(canon)
                    break

    # 1) —Å–Ω–∞—á–∞–ª–∞ —à–∞–ø–∫–∞ ‚Äî –∑–¥–µ—Å—å –æ–±—ã—á–Ω–æ —è–≤–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç: "–ú–ê–®–ò–ù–ê –ì–ï–ù–´", "–ê–í–¢–û–ë–£–°"
    scan_chunk(object_ or "")
    scan_chunk(subobject or "")

    # 2) –ø–æ—Ç–æ–º —Ç–µ–∫—Å—Ç —Å—Ü–µ–Ω—ã ‚Äî "–ø–æ–¥—ä–µ–∑–∂–∞–µ—Ç –∞–≤—Ç–æ–±—É—Å", "—Å–∞–¥—è—Ç—Å—è –≤ –º–∞—à–∏–Ω—É"
    scan_chunk(text or "")

    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–æ–Ω–æ–≤ (–≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä —É–∂–µ –∑–∞–ª–æ–∂–µ–Ω)
    return sorted(found)


def extract_grim_from_text(text: str, nlp) -> list[str]:
    """
    Rule-based –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥—Ä–∏–º–∞/–º–∞–∫–∏—è–∂–∞/—Ä–∞–Ω –Ω–∞ –ª–∏—Ü–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ü–µ–Ω—ã.
    –†–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –Ω–µ–π—Ä–æ–Ω–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ (—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö), —É–∂–µ —á–µ–ª–æ–≤–µ–∫–æ-–ø–æ–Ω—è—Ç–Ω—ã—Ö.
    """
    if not text:
        return []

    doc = nlp(text)
    candidates: set[str] = set()

    for token in doc:
        # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å—Ç—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é/—Ü–∏—Ñ—Ä—ã, –Ω–æ –ù–ï —Ä–µ–∂–µ–º –ø–æ –¥–µ—Ñ–∏—Å—É
        if not any(ch.isalpha() for ch in token.text):
            continue

        lemma_raw = ru_lemma(token.text)        # üëà pymorphy
        lemma = lemma_raw.replace("—ë", "–µ")
        text_norm = token.text.lower().replace("—ë", "–µ")

        # 1) –æ–±—ã—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞—Ä—é
        in_noun_dict = (lemma in GRIM_NOUN_LEMMAS_NORM)
        in_adj_dict  = lemma in GRIM_ADJ_LEMMAS_NORM

        # 2) —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–æ—Ä–Ω—é: –≤—Å—ë, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "—Ç–∞—Ç—É-"
        from_root = any(
            lemma.startswith(root) or text_norm.startswith(root)
            for root in GRIM_NOUN_LEMMAS
        )

        is_grim_noun = in_noun_dict or from_root
        is_grim_adj  = in_adj_dict

        if not (is_grim_noun or is_grim_adj):
            continue

        # --- —Å—Ç—Ä–æ–∏–º –Ω–µ–±–æ–ª—å—à—É—é —Ñ—Ä–∞–∑—É –≤–æ–∫—Ä—É–≥ —Ç–æ–∫–µ–Ω–∞ ---
        span_tokens = {token}

        # –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ, —Ü–µ–ø–ª—è–µ–º –≥–æ–ª–æ–≤—É-—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
        if is_grim_adj and token.head.pos_ in ("NOUN", "PROPN") and token.head.sent == token.sent:
            span_tokens.add(token.head)
            # –¥–æ–±–∞–≤–∏–º –µ—â—ë –¥—Ä—É–≥–∏–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫ —Ç–æ–º—É –∂–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º—É
            for ch in token.head.children:
                if ch.pos_ == "ADJ" and ch.sent == token.sent:
                    span_tokens.add(ch)

        # –µ—Å–ª–∏ —ç—Ç–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ, –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–ª–µ–≥–∞—é—â–∏–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
        if is_grim_noun and token.pos_ == "NOUN":
            for ch in token.children:
                if ch.pos_ == "ADJ" and ch.sent == token.sent:
                    span_tokens.add(ch)
            # –∏–Ω–æ–≥–¥–∞ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ —Å—Ç–æ–∏—Ç —Å–ª–µ–≤–∞ –∫–∞–∫ "—Ä–∞–∑–±–∏—Ç–∞—è –≥—É–±–∞"
            if token.i > 0:
                left = doc[token.i - 1]
                if left.pos_ == "ADJ" and left.sent == token.sent:
                    span_tokens.add(left)

        # —Å—Ç—Ä–æ–∏–º —Å–ø–∞–Ω
        start_i = min(t.i for t in span_tokens)
        end_i   = max(t.i for t in span_tokens) + 1
        span    = doc[start_i:end_i]
        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞: —Å—É—â., –ø—Ä–∏–ª., (–∏–Ω–æ–≥–¥–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–º–µ–Ω–∞)
        content_tokens = [
            t for t in span
            if t.is_alpha and t.pos_ in ("NOUN", "ADJ", "PROPN")
        ]

        if not content_tokens:
            continue

        # –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º ‚Üí –ø—Ä–∏–±–ª–∏–∂–∞–µ–º—Å—è –∫ "–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º—É –±–∞–∑–æ–≤–æ–º—É –≤–∏–¥—É"
        lemma_tokens = [
            t.lemma_.lower().replace("—ë", "–µ")
            for t in content_tokens
        ]

        lemma_phrase = " ".join(lemma_tokens).strip()
        if len(lemma_phrase) < 2:
            continue

        # –ø–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –∫–∞–∫ –µ—Å—Ç—å
        pretty = lemma_phrase[0].upper() + lemma_phrase[1:]
        candidates.add(pretty)

    # –Ω–µ–±–æ–ª—å—à–∞—è —á–∏—Å—Ç–∫–∞: —É–±–∏—Ä–∞–µ–º —Å–æ–≤—Å–µ–º –æ–±—â–∏–µ "–ö—Ä–æ–≤—å" / "–ì—Ä—è–∑—å", –µ—Å–ª–∏ –µ—Å—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    # (–æ—á–µ–Ω—å –º—è–≥–∫–æ, —á—Ç–æ–±—ã –Ω–∏—á–µ–≥–æ –Ω–µ –ª–æ–º–∞—Ç—å)
    filtered = set(candidates)
    for cand in list(candidates):
        low = cand.lower()
        if low in {"–∫—Ä–æ–≤—å", "–≥—Ä—è–∑—å"}:
            # –µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ-—Ç–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å —ç—Ç–∏–º —Å–ª–æ–≤–æ–º ‚Äî —É–±–∏—Ä–∞–µ–º –≥–æ–ª–æ–µ —Å–ª–æ–≤–æ
            if any(low in other.lower() and other != cand for other in candidates):
                filtered.discard(cand)

    return sorted(filtered)

def add_grim_column(df: pd.DataFrame, nlp) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∫–æ–ª–æ–Ω–∫—É '–ì—Ä–∏–º' –ø–æ rule-based-–ª–æ–≥–∏–∫–µ,
    –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–≥–Ω–æ—Ä–∏—Ä—É—è, —á—Ç–æ —Ç–∞–º –Ω–∞–≤—ã—á–∏—Å–ª—è–ª–∞ –Ω–µ–π—Ä–æ–Ω–∫–∞.
    """
    df = df.copy()
    values = []

    for _, row in df.iterrows():
        text = str(row.get("text", ""))
        grim_items = extract_grim_from_text(text, nlp)

        if grim_items:
            values.append("; ".join(grim_items))
        else:
            values.append("")

    df["–ì—Ä–∏–º"] = values
    return df

def inflect_adj_to_noun(adj_text: str, noun_text: str) -> str:
    """
    –°–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ —Å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º: —Ä–æ–¥/—á–∏—Å–ª–æ + –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π.
    –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –≤–µ—Ä–Ω—É—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ.
    """
    pa = morph.parse(adj_text)
    pn = morph.parse(noun_text)
    if not pa or not pn:
        return adj_text.lower()

    pa = pa[0]
    pn = pn[0]

    grammemes = {"nomn"}  # –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π
    # —Ä–æ–¥
    if "masc" in pn.tag:
        grammemes.add("masc")
    if "femn" in pn.tag:
        grammemes.add("femn")
    if "neut" in pn.tag:
        grammemes.add("neut")
    # —á–∏—Å–ª–æ
    if "plur" in pn.tag:
        grammemes.add("plur")
    if "sing" in pn.tag:
        grammemes.add("sing")

    inflected = pa.inflect(grammemes)
    if inflected:
        return inflected.word.lower()

    # fallback ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞
    return pa.normal_form.lower()

def extract_costume_from_text(text: str, nlp) -> list[str]:
    """
    Rule-based –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ—Å—Ç—é–º–∞ / –æ–¥–µ–∂–¥—ã / —Ñ–æ—Ä–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ü–µ–Ω—ã.
    –†–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –Ω–µ–π—Ä–æ–Ω–∫–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ (–≤ "–ø–æ—á—Ç–∏ –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º").
    """
    if not text:
        return []

    doc = nlp(text)
    candidates: set[str] = set()

    for token in doc:
        # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å—Ç—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é/—Ü–∏—Ñ—Ä—ã
        if not any(ch.isalpha() for ch in token.text):
            continue

        lemma_raw = ru_lemma(token.text)
        text_raw  = token.text.lower()

        lemma = lemma_raw.replace("—ë", "–µ")
        text_norm = text_raw.replace("—ë", "–µ")

        # 1) –æ–±—ã—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞—Ä—é
        in_noun_dict = (
            lemma in COSTUME_NOUN_LEMMAS_NORM     # "—Å—Ç—Ä–∏–Ω–≥" ‚àà {...}?
            or text_norm in COSTUME_NOUN_LEMMAS_NORM  # "—Å—Ç—Ä–∏–Ω–≥–∏" ‚àà {...}?
        )
        in_adj_dict  = lemma in COSTUME_ADJ_LEMMAS_NORM

        # 2) —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–æ—Ä–Ω—è–º (—Ñ–æ—Ä–º–∞/—É–Ω–∏—Ñ–æ—Ä–º–∞)
        from_root = any(
            lemma.startswith(root) or text_norm.startswith(root)
            for root in COSTUME_NOUN_ROOTS
        )

        is_costume_noun = in_noun_dict or from_root
        is_costume_adj  = in_adj_dict

        if not (is_costume_noun or is_costume_adj):
            continue

        if is_costume_adj:
            head = token.head
            if head.pos_ != "NOUN":
                continue

            head_lemma = ru_lemma(head.text).replace("—ë", "–µ")
            head_is_costume_noun = (
                head_lemma in COSTUME_NOUN_LEMMAS_NORM
                or any(head_lemma.startswith(root) for root in COSTUME_NOUN_ROOTS)
            )
            if not head_is_costume_noun:
                # '–ø–æ—Ö–æ–¥–Ω—ã–π –º–µ—Ç–æ–¥', '—Å—Ç–∞—Ä—ã–π –ø–æ—Ö–æ–¥–Ω—ã–π —Å–ø–æ—Å–æ–±' –∏ —Ç.–ø. ‚Äî –Ω–µ –∫–æ—Å—Ç—é–º
                continue

        # --- —Å—Ç—Ä–æ–∏–º —Å–ø–∞–Ω –≤–æ–∫—Ä—É–≥ –æ–ø–æ—Ä–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ ---
        span_tokens = {token}

        # –µ—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ ‚Äî –ø–æ–¥–Ω–∏–º–∞–µ–º—Å—è –∫ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º—É (—Ñ–æ—Ä–º–∞, –∫–æ—Å—Ç—é–º, –ø–ª–∞—Ç—å–µ‚Ä¶)
        if is_costume_adj and token.head.pos_ in ("NOUN", "PROPN") and token.head.sent == token.sent:
            span_tokens.add(token.head)
            for ch in token.head.children:
                if ch.pos_ == "ADJ" and ch.sent == token.sent:
                    span_tokens.add(ch)

        if is_costume_noun and token.pos_ == "NOUN":
            for ch in token.children:
                if ch.sent != token.sent:
                    continue

                # –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ "–∫–æ—Å—Ç—é–º–Ω—ã–µ" –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ ‚Äî –Ω–µ —Ç–∞—â–∏–º '–≤–∏–¥–Ω—ã–µ', '–ø–æ—Å–ª–µ–¥–Ω–∏–µ' –∏ —Ç.–ø.
                if ch.pos_ == "ADJ":
                    ch_lemma = ru_lemma(ch.text).replace("—ë", "–µ")
                    if ch_lemma in COSTUME_ADJ_LEMMAS_NORM:
                        span_tokens.add(ch)

                # –±—Ä–µ–Ω–¥ / –≤–ª–∞–¥–µ–ª–µ—Ü: –ú–∞–∫–¥–æ–Ω–∞–ª–¥—Å–∞, –ø–æ–ª–∏—Ü–∏–∏, –ì–ò–ë–î–î, –∏ —Ç.–ø.
                if ch.pos_ == "PROPN":
                    span_tokens.add(ch)

            # —Å–ª–µ–≤–∞ —Ç–æ–∂–µ —Ç–æ–ª—å–∫–æ "–∫–æ—Å—Ç—é–º–Ω—ã–µ" –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ
            if token.i > 0:
                left = doc[token.i - 1]
                if left.sent == token.sent and left.pos_ == "ADJ":
                    left_lemma = ru_lemma(left.text).replace("—ë", "–µ")
                    if left_lemma in COSTUME_ADJ_LEMMAS_NORM:
                        span_tokens.add(left)


        # —Å—Ç—Ä–æ–∏–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ø–∞–Ω–∞
        start_i = min(t.i for t in span_tokens)
        end_i   = max(t.i for t in span_tokens) + 1
        span    = doc[start_i:end_i]

        # –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è –∫ "–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º—É"
        content_tokens = [
            t for t in span
            if t.is_alpha and t.pos_ in ("NOUN", "ADJ", "PROPN")
        ]
        if not content_tokens:
            continue

        # –∏—â–µ–º –≥–ª–∞–≤–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –≤ —Å–ø–∞–Ω–µ
        head_noun = None
        for t in content_tokens:
            if t.pos_ == "NOUN":
                head_noun = t
                break

        head_noun_text = head_noun.text if head_noun is not None else None

        lemma_tokens = []
        for t in content_tokens:
            if t.pos_ == "PROPN":
                # –±—Ä–µ–Ω–¥—ã/–∏–º–µ–Ω–∞ ‚Äî –∫–∞–∫ –≤ —Ç–µ–∫—Å—Ç–µ
                tok = t.text
            elif t.pos_ == "NOUN":
                # —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ (–ò.–ø.)
                tok = ru_lemma(t.text)
            else:  # ADJ
                # –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ —Å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º
                if head_noun_text is not None:
                    tok = inflect_adj_to_noun(t.text, head_noun_text)
                else:
                    tok = ru_lemma(t.text)
            lemma_tokens.append(tok.lower().replace("—ë", "–µ"))

        lemma_phrase = " ".join(lemma_tokens).strip()
        if len(lemma_phrase) < 2:
            continue

        pretty = lemma_phrase[0].upper() + lemma_phrase[1:]
        candidates.add(pretty)

    return sorted(candidates)

def add_costume_column(df: pd.DataFrame, nlp) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∫–æ–ª–æ–Ω–∫—É '–ö–æ—Å—Ç—é–º' –ø–æ rule-based-–ª–æ–≥–∏–∫–µ,
    –∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ç–æ, —á—Ç–æ –≤—ã–¥–∞–ª–∞ –Ω–µ–π—Ä–æ–Ω–∫–∞.
    """
    df = df.copy()
    values = []

    for _, row in df.iterrows():
        text = str(row.get("text", ""))
        costume_items = extract_costume_from_text(text, nlp)

        if costume_items:
            values.append("; ".join(costume_items))
        else:
            values.append("")

    df["–ö–æ—Å—Ç—é–º"] = values
    return df

def _collect_keyword_spans(
    text: str,
    nlp,
    *,
    noun_lemmas: set[str] | None = None,
    adj_lemmas: set[str] | None = None,
    verb_lemmas: set[str] | None = None,
    window: int = 3,
) -> list[str]:
    """
    –û–±—â–∏–π —Ö–µ–ª–ø–µ—Ä:
      - –±–µ–≥–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º,
      - –∏—â–µ–º —Ç—Ä–∏–≥–≥–µ—Ä—ã –ø–æ –ª–µ–º–º–∞–º (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ / –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ / –≥–ª–∞–≥–æ–ª—ã),
      - —Ä–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∞–Ω –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è +- window —Ç–æ–∫–µ–Ω–æ–≤,
      - —Å–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (ADJ/NOUN/PROPN/VERB –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É),
      - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ "–∫—Ä–∞—Å–∏–≤–æ–≥–æ" —Ç–µ–∫—Å—Ç–∞.
    """
    if not text:
        return []

    doc = nlp(text)
    candidates: set[str] = set()

    noun_lemmas = noun_lemmas or set()
    adj_lemmas  = adj_lemmas or set()
    verb_lemmas = verb_lemmas or set()

    for sent in doc.sents:
        sent_tokens = list(sent)
        for i, tok in enumerate(sent_tokens):
            if not tok.is_alpha:
                continue
            form = tok.text.lower().replace("—ë", "–µ")
            lemma = ru_lemma(tok.text).replace("—ë", "–µ")

            is_trigger = False

            if noun_lemmas and lemma in noun_lemmas:
                is_trigger = True
            if adj_lemmas and lemma in adj_lemmas:
                is_trigger = True
            if verb_lemmas and lemma in verb_lemmas:
                is_trigger = True

            # –¥–æ–ø. —Ç—Ä–∏–≥–≥–µ—Ä –¥–ª—è —Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
            if not is_trigger and FX_KEYWORDS_N:
                for kw in FX_KEYWORDS_N:
                    if kw in sent.text.lower().replace("—ë", "–µ"):
                        is_trigger = True
                        break

            if not is_trigger:
                continue

            # —Å—Ç—Ä–æ–∏–º –º–∞–ª–µ–Ω—å–∫–æ–µ –æ–∫–Ω–æ –≤–æ–∫—Ä—É–≥ —Ç—Ä–∏–≥–≥–µ—Ä–∞
            left = max(0, i - window)
            right = min(len(sent_tokens), i + window + 1)
            span_tokens = sent_tokens[left:right]

            content = []
            for t in span_tokens:
                if not t.is_alpha:
                    continue
                if t.pos_ in ("DET", "PART", "CCONJ", "SCONJ", "ADP", "PRON"):
                    # –∞—Ä—Ç–∏–∫–ª–∏/—á–∞—Å—Ç–∏—Ü—ã/—Å–æ—é–∑—ã/–ø—Ä–µ–¥–ª–æ–≥–∏/–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –≤—ã–∫–∏–¥—ã–≤–∞–µ–º
                    continue
                content.append(t.text.lower().replace("—ë", "–µ"))

            if not content:
                continue

            phrase = " ".join(content)
            if len(phrase) < 2:
                continue

            pretty = phrase[0].upper() + phrase[1:]
            candidates.add(pretty)

    return sorted(candidates)

def extract_pyro_from_text(text: str, nlp) -> list[str]:
    """
    –ü–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –û–î–ù–û–°–õ–û–í–ù–´–ï –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
    (–∫–æ—Å—Ç–µ—Ä, —Ñ–µ–π–µ—Ä–≤–µ—Ä–∫, —Å–∞–ª—é—Ç, –ø–µ—Ç–∞—Ä–¥–∞ –∏ —Ç.–ø.), –±–µ–∑ –æ–∫–æ–Ω –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    if not text:
        return []

    doc = nlp(text)
    result: set[str] = set()

    for sent in doc.sents:
        tokens = list(sent)
        for i, tok in enumerate(tokens):
            if not tok.is_alpha:
                continue

            form = tok.text.lower().replace("—ë", "–µ")
            lemma = ru_lemma(tok.text).replace("—ë", "–µ")

            # —Ç—Ä–∏–≥–≥–µ—Ä –ø–æ —Å–ª–æ–≤–∞—Ä—é —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
            if lemma not in PYRO_NOUNS_N and form not in PYRO_NOUNS_N:
                continue

            # —Å–ø–µ—Ü-—Ñ–∏–ª—å—Ç—Ä: "–≤–∑—Ä—ã–≤ —Ö–æ—Ö–æ—Ç–∞/—Å–º–µ—Ö–∞" ‚Äî –Ω–µ –ø–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞
            if lemma == "–≤–∑—Ä—ã–≤":
                window = tokens[max(0, i - 3): i + 4]
                neigh_lemmas = {ru_lemma(t.text).lower() for t in window}
                if {"—Å–º–µ—Ö", "—Ö–æ—Ö–æ—Ç"} & neigh_lemmas:
                    continue

            # –∫–∞–Ω–æ–Ω: –æ–¥–Ω–∞ –ª–µ–º–º–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
            canon = lemma[0].upper() + lemma[1:]
            result.add(canon)

    return sorted(result)



def extract_fx_from_text(text: str, nlp) -> list[str]:
    """
    –°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã: –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ-–∫–∞–Ω–æ–Ω
    (–§–ª–µ—à–±–µ–∫, –¢—É–º–∞–Ω, –î—ã–º –∏ —Ç.–ø.), –±–µ–∑ —Ö–≤–æ—Å—Ç–æ–≤ —Ç–∏–ø–∞ '–ò–ù–¢', '–ö–∞—Ç–∏ –¥–æ–Ω–æ—Å–∏—Ç—Å—è –≥–æ–ª–æ—Å'.
    """
    # —Å–Ω–∞—á–∞–ª–∞ –±–µ—Ä—ë–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–∞–∫ —Ä–∞–Ω—å—à–µ ‚Äî –º–∞–ª–µ–Ω—å–∫–∏–µ —Ñ—Ä–∞–∑—ã –≤–æ–∫—Ä—É–≥ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
    phrases = _collect_keyword_spans(
        text,
        nlp,
        noun_lemmas=FX_NOUNS_N,
        verb_lemmas=None,
        adj_lemmas=None,
        window=4,
    )

    cleaned: set[str] = set()

    for ph in phrases:
        low = ph.lower().replace("—ë", "–µ").strip()
        if not low:
            continue

        # –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Ñ—Ä–∞–∑—ã
        first = low.split()[0]

        lemma = ru_lemma(first).replace("—ë", "–µ")

        # —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏, —á—Ç–æ–±—ã –±—ã–ª–æ –∫—Ä–∞—Å–∏–≤–æ
        if lemma in {"—Ñ–ª–µ—à–±–µ–∫", "flashback", "—Ñ–ª–µ—à–±—ç–∫"}:
            canon = "–§–ª–µ—à–±–µ–∫"
        elif lemma in {"—Ç—É–º–∞–Ω", "–¥—ã–º"}:
            canon = lemma.capitalize()
        else:
            # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –ø—Ä–æ—Å—Ç–æ –ª–µ–º–º–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
            canon = lemma[:1].upper() + lemma[1:] if lemma else ph

        cleaned.add(canon)

    return sorted(cleaned)


def extract_stunts_from_text(text: str, nlp) -> list[str]:
    """
    –ö–∞—Å–∫–∞–¥—ë—Ä—ã / –¥—É–±–ª—ë—Ä—ã –≤ –æ–ø–∏—Å–∞–Ω–∏–∏.
    """
    return _collect_keyword_spans(
        text,
        nlp,
        noun_lemmas=STUNT_WORDS_N,
        adj_lemmas=None,
        verb_lemmas=None,
        window=3,
    )

def _normalize_place_segment(seg: str) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º –∫—É—Å–æ—á–µ–∫ place/object/subobject –∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–º—É –≤–∏–¥—É:
    - —Ä–µ–∂–µ–º –ø–æ ¬´(—Å–º. —Å—Ü.8)¬ª –∏ —Ç.–ø.,
    - —á–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–æ—á–∫–∏,
    - –¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç–æ –°—Ç—Ä–æ—á–Ω–æ–µ –° –ó–∞–≥–ª–∞–≤–Ω–æ–π.
    """
    if not seg:
        return ""
    s = seg.strip()
    # —É–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ (–°–ú. –°–¶.8)
    s = re.sub(r"\(.*?\)", "", s)
    s = re.sub(r"\s+", " ", s)
    s = s.strip(" .:/-")
    if not s:
        return ""
    low = s.lower()
    # title –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –æ–∫, –∂–∏—Ç—å –º–æ–∂–Ω–æ
    return low[0].upper() + low[1:]


def extract_decoration_from_place(
    location: str,
    object_: str,
    subobject: str,
) -> list[str]:
    """
    –°—Ç—Ä–æ–∏–º '–î–µ–∫–æ—Ä–∞—Ü–∏—è' —Ç–æ–ª—å–∫–æ –∏–∑ Object / Subobject.
    location ‚Äî '–ò–ù–¢', '–ù–ê–¢', '–ò–ù–¢/–ù–ê–¢' –∏ —Ç.–ø.
    """
    loc = (location or "").upper().replace("–Å", "–ï")

    # —Ä–∞–∑–±–∏–≤–∞–µ–º object/subobject –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ —Ç–æ—á–∫–∞–º –∏ —Å–ª—ç—à–∞–º
    raw_segments = []

    for part in (object_ or "").split("/"):
        raw_segments.extend(p.strip() for p in part.split(".") if p.strip())

    for part in (subobject or "").split("/"):
        raw_segments.extend(p.strip() for p in part.split(".") if p.strip())

    decorations: set[str] = set()

    for seg in raw_segments:
        norm_seg = _normalize_place_segment(seg)
        if not norm_seg:
            continue

        low = norm_seg.lower()

        # –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç ‚Äî –æ–¥–Ω–æ –∏–∑ "–ì–û–†–û–î/–£–õ–ò–¶–ê/–ì–û–†–´/–†–ï–ö–ê" –∏ —Ç.–ø. ‚Üí –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if low in GENERIC_PLACE_STOP:
            continue

        # –µ—Å–ª–∏ –ª–æ–∫–∞—Ü–∏—è –ò–ù–¢ ‚Üí –ø–æ—á—Ç–∏ –≤—Å—ë, —á—Ç–æ –Ω–µ generic, –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å –¥–µ–∫–æ—Ä–∞—Ü–∏–µ–π
        if loc.startswith("–ò–ù–¢"):
            decorations.add(norm_seg)
            continue

        # –µ—Å–ª–∏ –ù–ê–¢ ‚Üí –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ —Ä—É–∫–æ—Ç–≤–æ—Ä–Ω—ã–µ –º–µ—Å—Ç–∞ (–ª–∞–≥–µ—Ä—å, —Å—Ç–∞–Ω—Ü–∏—è, –ø—Ä–∏—Å—Ç–∞–Ω—å‚Ä¶)
        if loc.startswith("–ù–ê–¢"):
            # –¥–ª—è NAT –¥–æ–ø—É—Å–∫–∞–µ–º –¥–µ–∫–æ—Ä–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç
            # —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á –∏–∑ MANMADE_PLACES
            if any(key in low for key in MANMADE_PLACES):
                decorations.add(norm_seg)
            continue

        # –Ω–∞ —Å–ª—É—á–∞–π –≠–ö–°–¢ / —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤:
        # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ –¥–ª—è –ù–ê–¢ ‚Äî —Ç–æ–ª—å–∫–æ MANMADE
        if any(key in low for key in MANMADE_PLACES):
            decorations.add(norm_seg)

    return sorted(decorations)

def add_decoration_from_place(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–æ–Ω–∫—É '–î–µ–∫–æ—Ä–∞—Ü–∏—è' —Ç–æ–ª—å–∫–æ –∏–∑
    '–ò–Ω—Ç / –Ω–∞—Ç' + '–û–±—ä–µ–∫—Ç' + '–ü–æ–¥–æ–±—ä–µ–∫—Ç'.
    """
    df = df.copy()
    decos = []

    for _, row in df.iterrows():
        location  = str(row.get("location", ""))
        object_   = str(row.get("object", ""))
        subobject = str(row.get("subobject", ""))

        items = extract_decoration_from_place(location, object_, subobject)
        decos.append("; ".join(items) if items else "")

    df["–î–µ–∫–æ—Ä–∞—Ü–∏—è"] = decos
    return df


def add_pyro_column(df: pd.DataFrame, nlp) -> pd.DataFrame:
    df = df.copy()
    vals = []
    for _, row in df.iterrows():
        text = str(row.get("text", ""))
        items = extract_pyro_from_text(text, nlp)
        vals.append("; ".join(items) if items else "")
    df["–ü–∏—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞"] = vals
    return df


def add_fx_column(df: pd.DataFrame, nlp) -> pd.DataFrame:
    df = df.copy()
    vals = []
    for _, row in df.iterrows():
        text = str(row.get("text", ""))
        items = extract_fx_from_text(text, nlp)
        vals.append("; ".join(items) if items else "")
    df["–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç"] = vals
    return df


def add_stunt_column(df: pd.DataFrame, nlp) -> pd.DataFrame:
    """
    –ö–∞—Å–∫–∞–¥—ë—Ä—ã: –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º rule-based –ø–æ —Ç–µ–∫—Å—Ç—É + –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –≤—ã–¥—ë—Ä–≥–∏–≤–∞–µ–º –∏–∑ '–ì—Ä—É–ø–ø–æ–≤–∫–∞'.
    """
    df = df.copy()
    vals = []

    for _, row in df.iterrows():
        text = str(row.get("text", ""))
        items = set(extract_stunts_from_text(text, nlp))

        # –µ—Å–ª–∏ —É —Ç–µ–±—è –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ '–ì—Ä—É–ø–ø–æ–≤–∫–∞' —Å–æ —Å—Ç—Ä–æ–∫–æ–π –≤–∏–¥–∞ "–¢–µ—Ö–Ω–∏–∫–∏ (2); –ö–∞—Å–∫–∞–¥–µ—Ä ‚Äì –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–∏–∫ 1"
        grp = str(row.get("–ì—Ä—É–ø–ø–æ–≤–∫–∞", ""))
        if grp:
            for chunk in re.split(r"[;,]", grp):
                if "–∫–∞—Å–∫–∞–¥–µ—Ä" in chunk.lower().replace("—ë", "–µ"):
                    cleaned = chunk.strip()
                    if cleaned:
                        items.add(cleaned)

        vals.append("; ".join(sorted(items)) if items else "")

    df["–ö–∞—Å–∫–∞–¥–µ—Ä"] = vals
    return df


# ==== 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ ====


def build_scenes_dataframe(script_path: str) -> pd.DataFrame:
    """
    1) –ß–∏—Ç–∞–µ—Ç docx/pdf.
    2) –†–µ–∂–µ—Ç –Ω–∞ —Å—Ü–µ–Ω—ã parse_script_with_episode.
    3) –î–ª—è –∫–∞–∂–¥–æ–π —Å—Ü–µ–Ω—ã –¥–æ—Å—Ç–∞—ë—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π / –º–∞—Å—Å–æ–≤–∫—É / –≥—Ä—É–ø–ø–æ–≤–∫—É.
    –ù–∞ –≤—ã—Ö–æ–¥–µ ‚Äî DataFrame –ø–æ —Å—Ü–µ–Ω–∞–º —Å –±–∞–∑–æ–≤—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ + 3 ¬´–≥–µ—Ä–æ–π—Å–∫–∏–º–∏¬ª.
    """
    # 1. —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤ —Ç–µ–∫—Å—Ç
    full_text = upload_file(script_path)

    # 2. —Ä–µ–∂–µ–º –Ω–∞ —Å—Ü–µ–Ω—ã –∏ –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    print('–†–µ–∂–µ–º –Ω–∞ —Å—Ü–µ–Ω—ã –∏ –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏.')
    scenes = parse_script_with_episode(full_text)

    # 3. –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º spaCy
    print('–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º spaCy –°–¢–ê–†–¢')
    nlp = load_ru()
    print('–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º spaCy –§–ò–ù–ò–®')

    rows = []
    for scene in scenes:
        scene_text = scene.get("text", "") or ""
        entities = extract_scene_entities(
                    scene_text,
                    object_=scene.get("object") or scene.get("–û–±—ä–µ–∫—Ç") or "",
                    subobject=scene.get("subobject") or scene.get("–ü–æ–¥–æ–±—ä–µ–∫—Ç") or "",
                    nlp=nlp,
                )
        characters = entities["characters"]
        grouping = entities["grouping"]
        massovka = entities["massovka"]

        # –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ ‚Äî –±–µ—Ä—ë–º –∫–∞–Ω–æ–Ω, –æ–±—ã—á–Ω–æ is_main=True
        char_names = sorted({c.canonical_name for c in characters})
        characters_str = "; ".join(char_names)

        grouping_str = "; ".join(grouping)
        massovka_str = "; ".join(massovka)

        row = dict(scene)
        row["–ü–µ—Ä—Å–æ–Ω–∞–∂–∏"] = characters_str
        row["–ì—Ä—É–ø–ø–æ–≤–∫–∞"] = grouping_str
        row["–ú–∞—Å—Å–æ–≤–∫–∞"] = massovka_str

        rows.append(row)

    df_scenes = pd.DataFrame(rows)
    return df_scenes


def run_nn_block(df_scenes: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:
    """
    4) –ü—Ä–æ–≥–æ–Ω—è–µ—Ç —Ç–µ –∂–µ —Å—Ü–µ–Ω—ã —á–µ—Ä–µ–∑ —Ç–≤–æ—é NER-–º–æ–¥–µ–ª—å –∏ process_dataframe.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ NN-–∫–æ–ª–æ–Ω–∫–∏ (+ ¬´text¬ª –≤–Ω—É—Ç—Ä–∏).
    """
    # –º–æ–¥–µ–ª—å –∏ device —É —Ç–µ–±—è —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –∏ –µ—Å—Ç—å MODEL_PATH / device
    model_local = AutoModelForTokenClassification.from_pretrained(MODEL_PATH).to(device)
    nlp = load_ru()
    df_ents = process_dataframe(df_scenes, model=model_local, nlp=nlp, text_col="text", threshold=threshold)
    return df_ents

def add_game_transport_column(df_scenes: pd.DataFrame, nlp) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –≤ df_scenes –∫–æ–ª–æ–Ω–∫—É "–ò–≥—Ä–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç"
    –Ω–∞ –æ—Å–Ω–æ–≤–µ object / subobject / text.
    """
    values = []
    for _, row in df_scenes.iterrows():
        obj = str(row.get("object", "") or row.get("–û–±—ä–µ–∫—Ç", "") or "")
        sub = str(row.get("subobject", "") or row.get("–ü–æ–¥–æ–±—ä–µ–∫—Ç", "") or "")
        txt = str(row.get("text", ""))

        canon_list = extract_game_transport_for_scene(obj, sub, txt, nlp)
        if canon_list:
            pretty = [c.capitalize() for c in canon_list]  # '–ú–ê–®–ò–ù–ê' ‚Üí '–ú–∞—à–∏–Ω–∞'
            values.append("; ".join(pretty))
        else:
            values.append("")

    df_scenes = df_scenes.copy()
    df_scenes["–ò–≥—Ä–æ–≤–æ–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç"] = values
    return df_scenes

def full_pipeline_1(script_path: str, threshold: float = 0.5) -> pd.DataFrame:
    """
    –ü–æ–ª–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç:
      —Ñ–∞–π–ª ‚Üí —Ç–µ–∫—Å—Ç ‚Üí —Å—Ü–µ–Ω—ã (4 –∫–æ–ª–æ–Ω–∫–∏) ‚Üí + –ü–µ—Ä—Å–æ–Ω–∞–∂–∏/–ì—Ä—É–ø–ø–æ–≤–∫–∞/–ú–∞—Å—Å–æ–≤–∫–∞ ‚Üí + NN-–∫–æ–ª–æ–Ω–∫–∏.
    """
    # 1) –±–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ + –ø–µ—Ä—Å–æ–Ω–∞–∂–∏/–º–∞—Å—Å–æ–≤–∫–∞/–≥—Ä—É–ø–ø–æ–≤–∫–∞
    df_scenes = build_scenes_dataframe(script_path)

    df_scenes = add_game_transport_column(df_scenes, nlp=nlp)

    # 2) NN-–±–ª–æ–∫ –ø–æ–≤–µ—Ä—Ö —Ç–æ–π –∂–µ —Ç–∞–±–ª–∏—Ü—ã
    print('NN-–±–ª–æ–∫ –ø–æ–≤–µ—Ä—Ö —Ç–æ–π –∂–µ —Ç–∞–±–ª–∏—Ü—ã')
    df_ents = run_nn_block(df_scenes, threshold=threshold)

    # 3) —Å–∫–ª–µ–∏–≤–∞–µ–º: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ü–µ–Ω + NN-–∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ –≤—Ç–æ—Ä–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ text)
    print('–°–∫–ª–µ–π–∫–∞')
    df_final = pd.concat(
        [
            df_scenes.reset_index(drop=True),
            df_ents.drop(columns=["text"], errors="ignore").reset_index(drop=True),
        ],
        axis=1,
    )

    df_final = add_grim_column(df_final, nlp=nlp)
    df_final = add_costume_column(df_final, nlp=nlp)
    df_final = add_decoration_from_place(df_final)
    df_final = add_pyro_column(df_final, nlp)
    df_final = add_fx_column(df_final, nlp)
    df_final = add_stunt_column(df_final, nlp)

    df_final = df_final.rename(columns={"episode_num": "–≠–ø–∏–∑–æ–¥", "scene_num": "–°—Ü–µ–Ω–∞", "location": "–ò–Ω—Ç/–ù–∞—Ç",
     "object": "–û–±—ä–µ–∫—Ç", "subobject": "–ü–æ–¥–æ–±—ä–µ–∫—Ç", "time": "–†–µ–∂–∏–º", "text": "–¢–µ–∫—Å—Ç", "–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç": "–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã"})
    col = df_final["–†–µ–∂–∏–º"]
    df_final = df_final.drop("–†–µ–∂–∏–º", axis=1)
    df_final.insert(3, "–†–µ–∂–∏–º", col)

    # 4) —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    print('–°–æ—Ö—Ä–∞–Ω—è–µ–º')
    #df_final.to_excel(output_path, index=False)
    print(f"–ì–æ—Ç–æ–≤–æ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω")

    return df_final

def full_pipeline(script_path: str, output_path: str, threshold: float = 0.5) -> pd.DataFrame:
    """
    –ü–æ–ª–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç:
      —Ñ–∞–π–ª ‚Üí —Ç–µ–∫—Å—Ç ‚Üí —Å—Ü–µ–Ω—ã (4 –∫–æ–ª–æ–Ω–∫–∏) ‚Üí + –ü–µ—Ä—Å–æ–Ω–∞–∂–∏/–ì—Ä—É–ø–ø–æ–≤–∫–∞/–ú–∞—Å—Å–æ–≤–∫–∞ ‚Üí + NN-–∫–æ–ª–æ–Ω–∫–∏.
    """
    # 1) –±–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ + –ø–µ—Ä—Å–æ–Ω–∞–∂–∏/–º–∞—Å—Å–æ–≤–∫–∞/–≥—Ä—É–ø–ø–æ–≤–∫–∞
    df_scenes = build_scenes_dataframe(script_path)

    df_scenes = add_game_transport_column(df_scenes, nlp=nlp)

    # 2) NN-–±–ª–æ–∫ –ø–æ–≤–µ—Ä—Ö —Ç–æ–π –∂–µ —Ç–∞–±–ª–∏—Ü—ã
    print('NN-–±–ª–æ–∫ –ø–æ–≤–µ—Ä—Ö —Ç–æ–π –∂–µ —Ç–∞–±–ª–∏—Ü—ã')
    df_ents = run_nn_block(df_scenes, threshold=threshold)

    # 3) —Å–∫–ª–µ–∏–≤–∞–µ–º: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ü–µ–Ω + NN-–∫–æ–ª–æ–Ω–∫–∏ (–±–µ–∑ –≤—Ç–æ—Ä–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ text)
    print('–°–∫–ª–µ–π–∫–∞')
    df_final = pd.concat(
        [
            df_scenes.reset_index(drop=True),
            df_ents.drop(columns=["text"], errors="ignore").reset_index(drop=True),
        ],
        axis=1,
    )

    df_final = add_grim_column(df_final, nlp=nlp)
    df_final = add_costume_column(df_final, nlp=nlp)
    df_final = add_decoration_from_place(df_final)
    df_final = add_pyro_column(df_final, nlp)
    df_final = add_fx_column(df_final, nlp)
    df_final = add_stunt_column(df_final, nlp)

    df_final = df_final.rename(columns={"episode_num": "–≠–ø–∏–∑–æ–¥", "scene_num": "–°—Ü–µ–Ω–∞", "location": "–ò–Ω—Ç/–ù–∞—Ç",
     "object": "–û–±—ä–µ–∫—Ç", "subobject": "–ü–æ–¥–æ–±—ä–µ–∫—Ç", "time": "–†–µ–∂–∏–º", "text": "–¢–µ–∫—Å—Ç", "–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç": "–°–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç—ã"})
    col = df_final["–†–µ–∂–∏–º"]
    df_final = df_final.drop("–†–µ–∂–∏–º", axis=1)
    df_final.insert(3, "–†–µ–∂–∏–º", col)

    # 4) —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    df_final.to_excel(output_path, index=False)
    print(f"–ì–æ—Ç–æ–≤–æ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω")

    return df_final


if __name__ == "__main__":

    log_file = resource_path("main.log")

    parser = argparse.ArgumentParser(description="–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏—è.")
    parser.add_argument("input", help="–ü—É—Ç—å –∫ .docx –∏–ª–∏ .pdf —Å—Ü–µ–Ω–∞—Ä–∏—é")
    parser.add_argument("output", help="–ü—É—Ç—å –∫ .xlsx —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º")
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="–ü–æ—Ä–æ–≥ –¥–ª—è score –≤ N–ù (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.5)",
    )

    try:
        with log_file.open("a", encoding="utf-8") as f:
            f.write(f"\n=== START ===\n")
            f.flush()
        args = parser.parse_args()
        full_pipeline(args.input, args.output, threshold=args.threshold)
    
    except Exception:
        with log_file.open("a", encoding="utf-8") as f:
            f.write(f"\n=== EXCEPTION ===\n")
            traceback.print_exc(file=f)
            f.flush()
        traceback.print_exc()
        sys.exit(1)